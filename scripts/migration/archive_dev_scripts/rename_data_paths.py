#!/usr/bin/env python3
"""
Rename Data Paths in MinIO

Rename the data paths so clean data has the primary name:
- Clean data: bronze/wallet_transactions_deduplicated/ ‚Üí bronze/wallet_transactions/
- Duplicate data: bronze/wallet_transactions/ ‚Üí bronze/wallet_transactions_with_dupes/

Author: Generated by Claude Code
Date: 2025-06-19
"""

import boto3
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    # MinIO client
    s3_client = boto3.client(
        's3',
        endpoint_url='http://localhost:9000',
        aws_access_key_id='minioadmin',
        aws_secret_access_key='minioadmin123',
        region_name='us-east-1'
    )
    
    bucket = 'solana-data'
    
    print("\n" + "="*60)
    print("RENAMING DATA PATHS")
    print("Making clean data the primary path")
    print("="*60)
    
    # Step 1: Rename old duplicate data to archive location
    print(f"\nüì¶ Step 1: Moving duplicate data to archive...")
    old_prefix = 'bronze/wallet_transactions/'
    archive_prefix = 'bronze/wallet_transactions_with_dupes/'
    
    try:
        # List all objects in the old path
        paginator = s3_client.get_paginator('list_objects_v2')
        old_objects = []
        
        for page in paginator.paginate(Bucket=bucket, Prefix=old_prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    old_objects.append(obj['Key'])
        
        if old_objects:
            logger.info(f"Found {len(old_objects):,} objects to move to archive")
            
            # Copy each object to new location
            for i, old_key in enumerate(old_objects):
                new_key = old_key.replace(old_prefix, archive_prefix, 1)
                
                # Copy object
                s3_client.copy_object(
                    Bucket=bucket,
                    CopySource={'Bucket': bucket, 'Key': old_key},
                    Key=new_key
                )
                
                if (i + 1) % 1000 == 0:
                    logger.info(f"Copied {i+1:,}/{len(old_objects):,} objects to archive")
            
            logger.info(f"‚úÖ Successfully copied all {len(old_objects):,} objects to archive")
            
            # Delete original objects
            logger.info("Deleting original duplicate data...")
            for i, old_key in enumerate(old_objects):
                s3_client.delete_object(Bucket=bucket, Key=old_key)
                
                if (i + 1) % 1000 == 0:
                    logger.info(f"Deleted {i+1:,}/{len(old_objects):,} original objects")
            
            logger.info(f"‚úÖ Successfully deleted all {len(old_objects):,} original objects")
        else:
            logger.info("No objects found in old path")
    
    except Exception as e:
        logger.error(f"Error moving duplicate data: {e}")
        return False
    
    # Step 2: Move clean data to primary location
    print(f"\nüßπ Step 2: Moving clean data to primary location...")
    clean_prefix = 'bronze/wallet_transactions_deduplicated/'
    primary_prefix = 'bronze/wallet_transactions/'
    
    try:
        # List all objects in the clean path
        clean_objects = []
        
        for page in paginator.paginate(Bucket=bucket, Prefix=clean_prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    clean_objects.append(obj['Key'])
        
        if clean_objects:
            logger.info(f"Found {len(clean_objects):,} clean objects to move to primary location")
            
            # Copy each object to primary location
            for i, old_key in enumerate(clean_objects):
                new_key = old_key.replace(clean_prefix, primary_prefix, 1)
                
                # Copy object
                s3_client.copy_object(
                    Bucket=bucket,
                    CopySource={'Bucket': bucket, 'Key': old_key},
                    Key=new_key
                )
                
                if (i + 1) % 500 == 0:
                    logger.info(f"Copied {i+1:,}/{len(clean_objects):,} objects to primary location")
            
            logger.info(f"‚úÖ Successfully copied all {len(clean_objects):,} objects to primary location")
            
            # Delete deduplicated path objects
            logger.info("Deleting temporary deduplicated path...")
            for i, old_key in enumerate(clean_objects):
                s3_client.delete_object(Bucket=bucket, Key=old_key)
                
                if (i + 1) % 500 == 0:
                    logger.info(f"Deleted {i+1:,}/{len(clean_objects):,} temporary objects")
            
            logger.info(f"‚úÖ Successfully deleted all {len(clean_objects):,} temporary objects")
        else:
            logger.info("No objects found in clean path")
    
    except Exception as e:
        logger.error(f"Error moving clean data: {e}")
        return False
    
    # Step 3: Verify final state
    print(f"\n‚úÖ Verification: Checking final state...")
    
    try:
        # Check primary path
        primary_count = 0
        for page in paginator.paginate(Bucket=bucket, Prefix=primary_prefix):
            if 'Contents' in page:
                primary_count += len(page['Contents'])
        
        # Check archive path
        archive_count = 0
        for page in paginator.paginate(Bucket=bucket, Prefix=archive_prefix):
            if 'Contents' in page:
                archive_count += len(page['Contents'])
        
        print(f"\nüìä FINAL STATE:")
        print(f"  Primary Path (bronze/wallet_transactions/): {primary_count:,} files")
        print(f"  Archive Path (bronze/wallet_transactions_with_dupes/): {archive_count:,} files")
        print(f"  Status: ‚úÖ Clean data is now primary")
        
        return primary_count > 0
        
    except Exception as e:
        logger.error(f"Error during verification: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nüéâ DATA PATH RENAME COMPLETED!")
        print(f"Clean deduplicated data is now at: bronze/wallet_transactions/")
        print(f"Archived duplicate data is at: bronze/wallet_transactions_with_dupes/")
    else:
        print(f"\n‚ùå Data path rename failed")
    exit(0 if success else 1)