#!/usr/bin/env python3
"""
PostgreSQL Bronze Wallet Trade History Schema Analysis

Analyzes wallet_trade_history table from solana_pipeline.bronze schema
to understand its structure before migration.

Author: Generated by Claude Code
Date: 2025-06-19
"""

import os
import logging
import psycopg2
import pandas as pd
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BronzeWalletTradeHistoryAnalyzer:
    """Analyzes bronze.wallet_trade_history table schema and data"""
    
    def __init__(self):
        """Initialize PostgreSQL connection using same config as existing migration script"""
        
        # PostgreSQL connection - solana_pipeline database
        self.pg_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': 'solana_pipeline',  # Updated database name
            'user': os.getenv('DB_USER', 'postgres'),
            'password': os.getenv('DB_PASSWORD', 'St0ck!adePG')
        }
        
        self.table_name = 'wallet_trade_history'
        self.schema_name = 'bronze'
        
    def test_connection(self):
        """Test PostgreSQL connection"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT version();")
                    version = cur.fetchone()[0]
                    logger.info(f"‚úÖ Connected to PostgreSQL: {version}")
                    
                    # Check if database exists and we can access it
                    cur.execute("SELECT current_database();")
                    db_name = cur.fetchone()[0]
                    logger.info(f"‚úÖ Connected to database: {db_name}")
                    
                    return True
        except Exception as e:
            logger.error(f"‚ùå Connection failed: {e}")
            return False
    
    def analyze_table_schema(self):
        """Analyze the bronze.wallet_trade_history table schema"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                # Get table schema information
                schema_query = """
                SELECT 
                    column_name,
                    data_type,
                    character_maximum_length,
                    is_nullable,
                    column_default,
                    ordinal_position
                FROM information_schema.columns 
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position;
                """
                
                df_schema = pd.read_sql_query(schema_query, conn, params=[self.schema_name, self.table_name])
                
                if df_schema.empty:
                    logger.error(f"‚ùå Table {self.schema_name}.{self.table_name} not found")
                    return None
                
                logger.info(f"‚úÖ Found table {self.schema_name}.{self.table_name} with {len(df_schema)} columns")
                
                # Get table size information
                count_query = f"SELECT COUNT(*) FROM {self.schema_name}.{self.table_name};"
                with conn.cursor() as cur:
                    cur.execute(count_query)
                    row_count = cur.fetchone()[0]
                
                # Get table size in bytes
                size_query = f"""
                SELECT pg_size_pretty(pg_total_relation_size('{self.schema_name}.{self.table_name}')) as size,
                       pg_total_relation_size('{self.schema_name}.{self.table_name}') as size_bytes;
                """
                with conn.cursor() as cur:
                    cur.execute(size_query)
                    size_result = cur.fetchone()
                    table_size = size_result[0] if size_result else "Unknown"
                    size_bytes = size_result[1] if size_result else 0
                
                return {
                    'schema': df_schema,
                    'row_count': row_count,
                    'table_size': table_size,
                    'size_bytes': size_bytes
                }
                
        except Exception as e:
            logger.error(f"‚ùå Schema analysis failed: {e}")
            return None
    
    def sample_data(self, limit=5):
        """Get sample data from the table"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                sample_query = f"""
                SELECT * FROM {self.schema_name}.{self.table_name}
                ORDER BY 1
                LIMIT {limit};
                """
                
                df_sample = pd.read_sql_query(sample_query, conn)
                return df_sample
                
        except Exception as e:
            logger.error(f"‚ùå Sample data extraction failed: {e}")
            return None
    
    def analyze_data_patterns(self):
        """Analyze data patterns and quality"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                # Get basic statistics - try different timestamp columns
                stats_queries = [
                    # Try with 'timestamp' column
                    """
                    SELECT 
                        COUNT(*) as total_rows,
                        COUNT(DISTINCT wallet_address) as unique_wallets,
                        MIN(timestamp) as earliest_record,
                        MAX(timestamp) as latest_record
                    FROM {schema}.{table}
                    WHERE timestamp IS NOT NULL;
                    """,
                    # Try with 'fetched_at' column
                    """
                    SELECT 
                        COUNT(*) as total_rows,
                        COUNT(DISTINCT wallet_address) as unique_wallets,
                        MIN(fetched_at) as earliest_record,
                        MAX(fetched_at) as latest_record
                    FROM {schema}.{table}
                    WHERE fetched_at IS NOT NULL;
                    """,
                    # Try with 'created_at' column
                    """
                    SELECT 
                        COUNT(*) as total_rows,
                        COUNT(DISTINCT wallet_address) as unique_wallets,
                        MIN(created_at) as earliest_record,
                        MAX(created_at) as latest_record
                    FROM {schema}.{table}
                    WHERE created_at IS NOT NULL;
                    """,
                    # Fallback - just counts
                    """
                    SELECT 
                        COUNT(*) as total_rows,
                        COUNT(DISTINCT wallet_address) as unique_wallets,
                        NULL as earliest_record,
                        NULL as latest_record
                    FROM {schema}.{table};
                    """
                ]
                
                for i, query_template in enumerate(stats_queries):
                    try:
                        query = query_template.format(schema=self.schema_name, table=self.table_name)
                        with conn.cursor() as cur:
                            cur.execute(query)
                            stats = cur.fetchone()
                            
                            if stats:
                                return {
                                    'total_rows': stats[0],
                                    'unique_wallets': stats[1],
                                    'earliest_record': stats[2],
                                    'latest_record': stats[3],
                                    'timestamp_column_used': ['timestamp', 'fetched_at', 'created_at', 'none'][i]
                                }
                    except Exception as e:
                        if i == len(stats_queries) - 1:  # Last query
                            logger.error(f"‚ùå All timestamp queries failed: {e}")
                        continue
                    
        except Exception as e:
            logger.error(f"‚ùå Data pattern analysis failed: {e}")
            return None
    
    def compare_with_previous_migration(self):
        """Compare schema with the previously migrated table"""
        print(f"\nüîÑ COMPARISON WITH PREVIOUS MIGRATION:")
        print(f"  Previous: solana.public.wallet_trade_history (321 records)")
        print(f"  Current:  solana_pipeline.bronze.wallet_trade_history")
        print(f"  Expected: Similar schema but potentially different source/format")
    
    def run_analysis(self):
        """Run complete analysis and report results"""
        print("\n" + "="*65)
        print("BRONZE WALLET TRADE HISTORY TABLE ANALYSIS")
        print("(solana_pipeline.bronze schema)")
        print("="*65)
        
        # Test connection
        if not self.test_connection():
            print("‚ùå Cannot connect to PostgreSQL database")
            return False
        
        # Analyze schema
        schema_info = self.analyze_table_schema()
        if not schema_info:
            print("‚ùå Cannot analyze table schema")
            return False
        
        # Print schema details
        print(f"\nüìä TABLE INFORMATION:")
        print(f"  Database: {self.pg_config['database']}")
        print(f"  Schema: {self.schema_name}")
        print(f"  Table: {self.table_name}")
        print(f"  Total Rows: {schema_info['row_count']:,}")
        print(f"  Table Size: {schema_info['table_size']}")
        print(f"  Columns: {len(schema_info['schema'])}")
        
        # Print column details
        print(f"\nüìã COLUMN SCHEMA:")
        for _, row in schema_info['schema'].iterrows():
            nullable = "NULL" if row['is_nullable'] == 'YES' else "NOT NULL"
            max_length = f"({row['character_maximum_length']})" if pd.notna(row['character_maximum_length']) else ""
            default = f" DEFAULT {row['column_default']}" if pd.notna(row['column_default']) else ""
            marker = " üî∏" if any(keyword in row['column_name'].lower() for keyword in ['price', 'amount', 'value', 'usd', 'symbol', 'address']) else ""
            print(f"  {row['ordinal_position']:2d}. {row['column_name']:<25} | {row['data_type']}{max_length:<10} | {nullable}{default}{marker}")
        
        # Get sample data
        sample_data = self.sample_data()
        if sample_data is not None and not sample_data.empty:
            print(f"\nüìù SAMPLE DATA (first 5 rows, first 5 columns):")
            # Print column headers
            headers = list(sample_data.columns)
            print("  " + " | ".join(f"{col:<15}" for col in headers[:5]))
            print("  " + "-" * (16 * min(5, len(headers)) - 2))
            
            # Print sample rows
            for idx, row in sample_data.head().iterrows():
                values = [str(row[col])[:15] for col in headers[:5]]
                print("  " + " | ".join(f"{val:<15}" for val in values))
        
        # Analyze data patterns
        patterns = self.analyze_data_patterns()
        if patterns:
            print(f"\nüìà DATA PATTERNS:")
            print(f"  Total Records: {patterns['total_rows']:,}")
            print(f"  Unique Wallets: {patterns['unique_wallets']:,}")
            print(f"  Timestamp Column: {patterns['timestamp_column_used']}")
            if patterns['earliest_record'] and patterns['latest_record']:
                print(f"  Date Range: {patterns['earliest_record']} to {patterns['latest_record']}")
                date_range = patterns['latest_record'] - patterns['earliest_record']
                print(f"  Time Span: {date_range.days} days")
        
        # Compare with previous migration
        self.compare_with_previous_migration()
        
        print(f"\n‚úÖ Analysis complete. Ready for migration planning.")
        return True

if __name__ == "__main__":
    analyzer = BronzeWalletTradeHistoryAnalyzer()
    success = analyzer.run_analysis()
    exit(0 if success else 1)