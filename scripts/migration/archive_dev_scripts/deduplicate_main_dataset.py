#!/usr/bin/env python3
"""
Main Dataset Deduplication Script

Removes duplicate transactions from the main migrated dataset (3.6M records).
For duplicates, keeps the record with the most recent timestamp.

Processes: s3://solana-data/bronze/wallet_trade_history_bronze_fixed/

Deduplication Logic:
- Groups by: (wallet_address, transaction_hash)
- Keeps: Record with MAX(timestamp) 
- Removes: All other duplicates

Author: Generated by Claude Code
Date: 2025-06-19
"""

import os
import time
import logging
from datetime import datetime
import duckdb
import boto3
from botocore.exceptions import ClientError

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MainDatasetDeduplicator:
    """Deduplicates the main wallet transaction dataset"""
    
    def __init__(self):
        """Initialize DuckDB and MinIO connections"""
        
        # DuckDB connection with S3 configuration
        self.conn = duckdb.connect(':memory:')
        self.conn.execute("""
            INSTALL httpfs; LOAD httpfs;
            SET s3_endpoint='localhost:9000';
            SET s3_access_key_id='minioadmin';
            SET s3_secret_access_key='minioadmin123';
            SET s3_use_ssl=false;
            SET s3_url_style='path';
        """)
        
        # Test connection first
        try:
            test_result = self.conn.execute("SELECT 1").fetchone()
            logger.info("DuckDB connection successful")
        except Exception as e:
            logger.error(f"DuckDB connection failed: {e}")
        
        # MinIO client for output
        self.s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin123',
            region_name='us-east-1'
        )
        
        # Data locations
        self.source_path = 's3://solana-data/bronze/wallet_trade_history_bronze_fixed/**/*.parquet'
        self.bucket = 'solana-data'
        self.output_path = 'bronze/wallet_transactions_clean/'
        
    def analyze_duplicates(self):
        """Analyze duplicate patterns in the main dataset"""
        print("\n" + "="*70)
        print("DUPLICATE ANALYSIS - MAIN DATASET (3.6M RECORDS)")
        print("="*70)
        
        try:
            # Get overall statistics
            stats = self.conn.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT transaction_hash) as unique_transactions,
                    COUNT(DISTINCT CONCAT(wallet_address, '|', transaction_hash)) as unique_wallet_tx_pairs
                FROM read_parquet('{self.source_path}')
            """).fetchone()
            
            print(f"\n📊 DATASET STATISTICS:")
            print(f"  Total Records: {stats[0]:,}")
            print(f"  Unique Wallets: {stats[1]:,}")
            print(f"  Unique Transaction Hashes: {stats[2]:,}")
            print(f"  Unique (Wallet, TxHash) Pairs: {stats[3]:,}")
            
            # Check for duplicates
            duplicates_exist = stats[0] > stats[3]
            
            if duplicates_exist:
                # Get duplicate statistics
                duplicate_stats = self.conn.execute(f"""
                    SELECT 
                        COUNT(*) as duplicate_pairs,
                        SUM(duplicate_count - 1) as records_to_remove
                    FROM (
                        SELECT 
                            wallet_address,
                            transaction_hash,
                            COUNT(*) as duplicate_count
                        FROM read_parquet('{self.source_path}')
                        GROUP BY wallet_address, transaction_hash
                        HAVING COUNT(*) > 1
                    )
                """).fetchone()
                
                print(f"\n⚠️  DUPLICATES FOUND:")
                print(f"  Duplicate (Wallet, TxHash) Pairs: {duplicate_stats[0]:,}")
                print(f"  Records to Remove: {duplicate_stats[1]:,}")
                print(f"  Deduplication Rate: {duplicate_stats[1]/stats[0]*100:.2f}%")
                
                # Show examples of duplicates
                examples = self.conn.execute(f"""
                    SELECT 
                        wallet_address,
                        transaction_hash,
                        COUNT(*) as count,
                        MIN(timestamp) as earliest,
                        MAX(timestamp) as latest
                    FROM read_parquet('{self.source_path}')
                    GROUP BY wallet_address, transaction_hash
                    HAVING COUNT(*) > 1
                    ORDER BY COUNT(*) DESC
                    LIMIT 5
                """).fetchall()
                
                if examples:
                    print(f"\n🔍 DUPLICATE EXAMPLES:")
                    for ex in examples:
                        wallet_short = ex[0][:12] + "..." if len(ex[0]) > 15 else ex[0]
                        tx_short = ex[1][:12] + "..." if len(ex[1]) > 15 else ex[1]
                        print(f"  {wallet_short} | {tx_short} | Count: {ex[2]} | {ex[3]} to {ex[4]}")
                
                return True
            else:
                print(f"\n✅ NO DUPLICATES FOUND")
                print(f"  Dataset is already clean")
                return False
                
        except Exception as e:
            logger.error(f"Duplicate analysis failed: {e}")
            return False
    
    def deduplicate_and_export(self):
        """Remove duplicates and export in one operation"""
        try:
            print(f"\n🔄 DEDUPLICATION & EXPORT:")
            print(f"  Strategy: Keep record with most recent timestamp")
            
            # Ensure bucket exists
            try:
                self.s3_client.head_bucket(Bucket=self.bucket)
            except ClientError:
                self.s3_client.create_bucket(Bucket=self.bucket)
                logger.info(f"Created bucket: {self.bucket}")
            
            # Get unique dates for processing
            dates = self.conn.execute(f"""
                SELECT DISTINCT partition_date
                FROM read_parquet('{self.source_path}')
                WHERE partition_date IS NOT NULL
                ORDER BY partition_date
            """).fetchall()
            
            total_exported = 0
            
            print(f"  Processing {len(dates):,} unique dates...")
            
            for i, date_tuple in enumerate(dates):
                partition_date = date_tuple[0]
                date_str = partition_date.strftime('%Y-%m-%d') if partition_date else 'unknown'
                
                # Deduplicate and export for this date
                parquet_key = f"{self.output_path}date={date_str}/clean_{int(time.time())}_{i:06d}.parquet"
                
                try:
                    # Deduplicate and export in one operation
                    self.conn.execute(f"""
                        COPY (
                            SELECT *
                            FROM (
                                SELECT *,
                                    ROW_NUMBER() OVER (
                                        PARTITION BY wallet_address, transaction_hash 
                                        ORDER BY timestamp DESC
                                    ) as rn
                                FROM read_parquet('{self.source_path}')
                                WHERE partition_date = '{partition_date}'
                            ) ranked
                            WHERE rn = 1
                        ) TO 's3://{self.bucket}/{parquet_key}' 
                        (FORMAT PARQUET, COMPRESSION 'snappy')
                    """)
                    
                    # Count deduplicated records for this date
                    date_count = self.conn.execute(f"""
                        SELECT COUNT(*)
                        FROM (
                            SELECT *,
                                ROW_NUMBER() OVER (
                                    PARTITION BY wallet_address, transaction_hash 
                                    ORDER BY timestamp DESC
                                ) as rn
                            FROM read_parquet('{self.source_path}')
                            WHERE partition_date = '{partition_date}'
                        ) ranked
                        WHERE rn = 1
                    """).fetchone()[0]
                    
                    total_exported += date_count
                    
                    if i % 100 == 0:  # Progress update every 100 dates
                        progress = (i + 1) / len(dates) * 100
                        logger.info(f"Progress: {progress:.1f}% ({i+1:,}/{len(dates):,} dates) - {total_exported:,} records")
                    
                except Exception as e:
                    logger.error(f"Failed to process date {date_str}: {e}")
                    continue
            
            print(f"  Exported {total_exported:,} deduplicated records")
            return total_exported
            
        except Exception as e:
            logger.error(f"Deduplication and export failed: {e}")
            return 0
    
    def validate_deduplication(self):
        """Validate the deduplicated dataset"""
        try:
            print(f"\n✅ VALIDATION:")
            
            # Check for remaining duplicates
            remaining_dupes = self.conn.execute(f"""
                SELECT COUNT(*) as duplicate_pairs
                FROM (
                    SELECT wallet_address, transaction_hash, COUNT(*) as cnt
                    FROM read_parquet('s3://{self.bucket}/{self.output_path}**/*.parquet')
                    GROUP BY wallet_address, transaction_hash
                    HAVING COUNT(*) > 1
                )
            """).fetchone()[0]
            
            # Get final statistics
            final_stats = self.conn.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT transaction_hash) as unique_transactions,
                    MIN(timestamp) as earliest,
                    MAX(timestamp) as latest
                FROM read_parquet('s3://{self.bucket}/{self.output_path}**/*.parquet')
            """).fetchone()
            
            print(f"  Remaining Duplicates: {remaining_dupes:,}")
            print(f"  Final Record Count: {final_stats[0]:,}")
            print(f"  Unique Wallets: {final_stats[1]:,}")
            print(f"  Unique Transactions: {final_stats[2]:,}")
            print(f"  Date Range: {final_stats[3]} to {final_stats[4]}")
            
            success = remaining_dupes == 0
            print(f"  Status: {'✅ CLEAN' if success else '❌ DUPLICATES REMAIN'}")
            
            return success, final_stats[0]
            
        except Exception as e:
            logger.error(f"Validation failed: {e}")
            return False, 0
    
    def run_deduplication(self):
        """Execute complete deduplication process"""
        start_time = time.time()
        
        print("\n" + "="*70)
        print("MAIN DATASET DEDUPLICATION")
        print("Processing 3.6M+ Records from PostgreSQL Migration")
        print("="*70)
        
        # Step 1: Analyze duplicates
        has_duplicates = self.analyze_duplicates()
        
        if not has_duplicates:
            print(f"\n🎉 No deduplication needed - dataset is already clean!")
            return True
        
        # Step 2: Deduplicate and export
        print(f"\n🚀 STARTING DEDUPLICATION...")
        exported_count = self.deduplicate_and_export()
        
        if exported_count == 0:
            print(f"❌ Deduplication failed")
            return False
        
        # Step 3: Validate results
        validation_success, final_count = self.validate_deduplication()
        
        # Final summary
        total_time = time.time() - start_time
        
        print(f"\n🎉 DEDUPLICATION COMPLETED!")
        print(f"📊 FINAL SUMMARY:")
        print(f"  Clean Records: {final_count:,}")
        print(f"  Processing Time: {total_time/60:.1f} minutes")
        print(f"  Validation: {'✅ PASSED' if validation_success else '❌ FAILED'}")
        print(f"  Output Location: s3://{self.bucket}/{self.output_path}")
        print(f"\n🎯 READY FOR PNL PROCESSING")
        
        return validation_success
    
    def __del__(self):
        """Clean up connections"""
        if hasattr(self, 'conn'):
            self.conn.close()

if __name__ == "__main__":
    deduplicator = MainDatasetDeduplicator()
    success = deduplicator.run_deduplication()
    exit(0 if success else 1)