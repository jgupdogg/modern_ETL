#!/usr/bin/env python3
"""
Test Deduplication on Small Sample

Tests the deduplication logic on a small sample of files
to validate the approach before processing the full 3.6M records.

Author: Generated by Claude Code
Date: 2025-06-19
"""

import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import io
import time
import logging
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    # MinIO client
    s3_client = boto3.client(
        's3',
        endpoint_url='http://localhost:9000',
        aws_access_key_id='minioadmin',
        aws_secret_access_key='minioadmin123',
        region_name='us-east-1'
    )
    
    bucket = 'solana-data'
    prefix = 'bronze/wallet_trade_history_bronze_fixed/'
    
    print("\n" + "="*60)
    print("DEDUPLICATION TEST - SMALL SAMPLE")
    print("="*60)
    
    # Get first 20 files for testing
    try:
        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=20)
        
        if 'Contents' not in response:
            print("‚ùå No files found")
            return
        
        files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]
        print(f"üìÅ Processing {len(files)} sample files...")
        
        # Read all records from sample files
        all_records = []
        
        for file_key in files:
            try:
                obj_response = s3_client.get_object(Bucket=bucket, Key=file_key)
                parquet_buffer = io.BytesIO(obj_response['Body'].read())
                table = pq.read_table(parquet_buffer)
                df = table.to_pandas()
                
                for _, row in df.iterrows():
                    record = {
                        'wallet_address': row['wallet_address'],
                        'transaction_hash': row['transaction_hash'],
                        'timestamp': row['timestamp'],
                        'file_source': file_key,
                        'full_record': row.to_dict()
                    }
                    all_records.append(record)
                    
            except Exception as e:
                logger.error(f"Failed to read {file_key}: {e}")
                continue
        
        print(f"üìä SAMPLE ANALYSIS:")
        print(f"  Total Records: {len(all_records):,}")
        
        # Group by (wallet_address, transaction_hash)
        groups = defaultdict(list)
        for record in all_records:
            key = (record['wallet_address'], record['transaction_hash'])
            groups[key].append(record)
        
        # Find duplicates
        duplicates = []
        unique_pairs = 0
        
        for key, group in groups.items():
            if len(group) > 1:
                duplicates.append((key, group))
            unique_pairs += 1
        
        print(f"  Unique (Wallet, TxHash) Pairs: {unique_pairs:,}")
        print(f"  Duplicate Groups: {len(duplicates):,}")
        
        if duplicates:
            total_duplicate_records = sum(len(group) - 1 for _, group in duplicates)
            print(f"  Records to Remove: {total_duplicate_records:,}")
            print(f"  Deduplication Rate: {total_duplicate_records/len(all_records)*100:.2f}%")
            
            print(f"\nüîç DUPLICATE EXAMPLES:")
            for i, (key, group) in enumerate(duplicates[:3]):
                wallet_short = key[0][:12] + "..."
                tx_short = key[1][:12] + "..."
                print(f"\n  Group {i+1}: {wallet_short} | {tx_short}")
                
                # Sort by timestamp to see which would be kept
                group.sort(key=lambda x: x['timestamp'], reverse=True)
                
                for j, record in enumerate(group):
                    status = "‚úÖ KEEP" if j == 0 else "‚ùå REMOVE"
                    source_file = record['file_source'].split('/')[-1]
                    print(f"    {status} | {record['timestamp']} | {source_file}")
            
            # Test deduplication
            print(f"\nüîÑ TESTING DEDUPLICATION...")
            
            deduplicated = []
            for key, group in groups.items():
                # Sort by timestamp and keep the latest
                group.sort(key=lambda x: x['timestamp'], reverse=True)
                deduplicated.append(group[0]['full_record'])
            
            print(f"  Original Records: {len(all_records):,}")
            print(f"  After Deduplication: {len(deduplicated):,}")
            print(f"  Removed: {len(all_records) - len(deduplicated):,}")
            
            # Verify no duplicates remain
            final_pairs = set()
            for record in deduplicated:
                pair = (record['wallet_address'], record['transaction_hash'])
                if pair in final_pairs:
                    print(f"‚ùå ERROR: Duplicate still exists!")
                    return
                final_pairs.add(pair)
            
            print(f"‚úÖ DEDUPLICATION TEST SUCCESSFUL")
            print(f"  No duplicates remain in sample")
            
        else:
            print(f"‚úÖ NO DUPLICATES IN SAMPLE")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()