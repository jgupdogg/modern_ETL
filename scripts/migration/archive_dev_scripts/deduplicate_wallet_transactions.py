#!/usr/bin/env python3
"""
Wallet Transaction Deduplication Script

Removes duplicate transactions from the complete migrated dataset in MinIO.
For duplicates, keeps the record with the most recent timestamp.

Processes all migrated wallet transaction data:
- s3://solana-data/bronze/wallet_trade_history_public/ (321 records)
- s3://solana-data/bronze/wallet_trade_history_bronze_fixed/ (3.6M records)

Deduplication Logic:
- Groups by: (wallet_address, transaction_hash)
- Keeps: Record with MAX(timestamp) 
- Removes: All other duplicates

Output: Clean deduplicated dataset in s3://solana-data/bronze/wallet_transactions_deduplicated/

Author: Generated by Claude Code
Date: 2025-06-19
"""

import os
import time
import logging
from datetime import datetime
import duckdb
import boto3
from botocore.exceptions import ClientError

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('deduplication.log')
    ]
)
logger = logging.getLogger(__name__)

class WalletTransactionDeduplicator:
    """Deduplicates wallet transactions across all migrated datasets"""
    
    def __init__(self):
        """Initialize DuckDB and MinIO connections"""
        
        # DuckDB connection with S3 configuration
        self.conn = duckdb.connect(':memory:')
        self.conn.execute("""
            INSTALL httpfs; LOAD httpfs;
            SET s3_endpoint='localhost:9000';
            SET s3_access_key_id='minioadmin';
            SET s3_secret_access_key='minioadmin123';
            SET s3_use_ssl=false;
            SET s3_url_style='path';
        """)
        
        # MinIO client for output
        self.s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin123',
            region_name='us-east-1'
        )
        
        # Data locations - check what's actually available
        self.source_paths = [
            's3://solana-data/bronze/wallet_trade_history_bronze/**/*.parquet',
            's3://solana-data/bronze/wallet_trade_history_bronze_fixed/**/*.parquet',
            's3://solana-data/bronze/wallet_transactions/**/*.parquet'
        ]
        
        self.bucket = 'solana-data'
        self.output_path = 'bronze/wallet_transactions_deduplicated/'
        
    def analyze_duplicates(self):
        """Analyze duplicate patterns in the dataset"""
        print("\n" + "="*70)
        print("DUPLICATE ANALYSIS")
        print("="*70)
        
        try:
            # Create combined view of all data
            self.conn.execute("""
                CREATE OR REPLACE VIEW all_transactions AS
                SELECT 
                    wallet_address,
                    transaction_hash,
                    timestamp,
                    migration_source,
                    partition_date,
                    ROW_NUMBER() OVER (
                        PARTITION BY wallet_address, transaction_hash 
                        ORDER BY timestamp DESC, migration_timestamp DESC
                    ) as rn
                FROM read_parquet([
                    's3://solana-data/bronze/wallet_trade_history_bronze/**/*.parquet',
                    's3://solana-data/bronze/wallet_trade_history_bronze_fixed/**/*.parquet',
                    's3://solana-data/bronze/wallet_transactions/**/*.parquet'
                ])
            """)
            
            # Get overall statistics
            stats = self.conn.execute("""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT transaction_hash) as unique_transactions,
                    COUNT(DISTINCT CONCAT(wallet_address, '|', transaction_hash)) as unique_wallet_tx_pairs
                FROM all_transactions
            """).fetchone()
            
            # Get duplicate statistics
            duplicate_stats = self.conn.execute("""
                SELECT 
                    COUNT(*) as duplicate_pairs,
                    SUM(duplicate_count - 1) as records_to_remove
                FROM (
                    SELECT 
                        wallet_address,
                        transaction_hash,
                        COUNT(*) as duplicate_count
                    FROM all_transactions
                    GROUP BY wallet_address, transaction_hash
                    HAVING COUNT(*) > 1
                )
            """).fetchone()
            
            print(f"\nüìä DATASET STATISTICS:")
            print(f"  Total Records: {stats[0]:,}")
            print(f"  Unique Wallets: {stats[1]:,}")
            print(f"  Unique Transaction Hashes: {stats[2]:,}")
            print(f"  Unique (Wallet, TxHash) Pairs: {stats[3]:,}")
            
            if duplicate_stats[0] and duplicate_stats[0] > 0:
                print(f"\n‚ö†Ô∏è  DUPLICATES FOUND:")
                print(f"  Duplicate (Wallet, TxHash) Pairs: {duplicate_stats[0]:,}")
                print(f"  Records to Remove: {duplicate_stats[1]:,}")
                print(f"  Deduplication Rate: {duplicate_stats[1]/stats[0]*100:.2f}%")
                
                # Show examples of duplicates
                examples = self.conn.execute("""
                    SELECT 
                        wallet_address,
                        transaction_hash,
                        timestamp,
                        migration_source,
                        COUNT(*) as count
                    FROM all_transactions
                    GROUP BY wallet_address, transaction_hash, timestamp, migration_source
                    HAVING COUNT(*) > 1
                    ORDER BY COUNT(*) DESC
                    LIMIT 5
                """).fetchall()
                
                if examples:
                    print(f"\nüîç DUPLICATE EXAMPLES:")
                    for ex in examples:
                        wallet_short = ex[0][:12] + "..." if len(ex[0]) > 15 else ex[0]
                        tx_short = ex[1][:12] + "..." if len(ex[1]) > 15 else ex[1]
                        print(f"  {wallet_short} | {tx_short} | {ex[2]} | {ex[3]} | Count: {ex[4]}")
                
                return True
            else:
                print(f"\n‚úÖ NO DUPLICATES FOUND")
                print(f"  Dataset is already clean")
                return False
                
        except Exception as e:
            logger.error(f"Duplicate analysis failed: {e}")
            return False
    
    def deduplicate_data(self):
        """Remove duplicates, keeping the most recent timestamp"""
        try:
            print(f"\nüîÑ DEDUPLICATION PROCESS:")
            print(f"  Strategy: Keep record with most recent timestamp")
            print(f"  Tie-breaker: Most recent migration_timestamp")
            
            # Create deduplicated view
            self.conn.execute("""
                CREATE OR REPLACE VIEW deduplicated_transactions AS
                SELECT 
                    wallet_address,
                    transaction_hash,
                    timestamp,
                    block_slot,
                    token_a,
                    token_b,
                    amount_a,
                    amount_b,
                    value_usd,
                    base_price,
                    quote_price,
                    nearest_price,
                    transaction_type,
                    transaction_fee,
                    success,
                    error_message,
                    processed_at,
                    batch_id,
                    processed_for_pnl,
                    pnl_processed_at,
                    pnl_processing_batch_id,
                    token_a_address,
                    token_b_address,
                    token_a_decimals,
                    token_b_decimals,
                    schema_version,
                    migration_source,
                    migration_timestamp,
                    partition_date
                FROM (
                    SELECT *,
                        ROW_NUMBER() OVER (
                            PARTITION BY wallet_address, transaction_hash 
                            ORDER BY timestamp DESC, migration_timestamp DESC
                        ) as rn
                    FROM read_parquet([
                        's3://solana-data/bronze/wallet_trade_history_bronze/**/*.parquet',
                        's3://solana-data/bronze/wallet_trade_history_bronze_fixed/**/*.parquet',
                        's3://solana-data/bronze/wallet_transactions/**/*.parquet'
                    ])
                ) ranked
                WHERE rn = 1
            """)
            
            # Get deduplicated count
            dedup_count = self.conn.execute("""
                SELECT COUNT(*) FROM deduplicated_transactions
            """).fetchone()[0]
            
            logger.info(f"Deduplicated dataset contains {dedup_count:,} records")
            return dedup_count
            
        except Exception as e:
            logger.error(f"Deduplication failed: {e}")
            return None
    
    def export_deduplicated_data(self):
        """Export deduplicated data to MinIO with date partitioning"""
        try:
            print(f"\nüíæ EXPORTING DEDUPLICATED DATA:")
            
            # Ensure bucket exists
            try:
                self.s3_client.head_bucket(Bucket=self.bucket)
            except ClientError:
                self.s3_client.create_bucket(Bucket=self.bucket)
                logger.info(f"Created bucket: {self.bucket}")
            
            # Get unique dates for partitioned export
            dates = self.conn.execute("""
                SELECT DISTINCT partition_date
                FROM deduplicated_transactions
                ORDER BY partition_date
            """).fetchall()
            
            total_exported = 0
            
            for date_tuple in dates:
                partition_date = date_tuple[0]
                date_str = partition_date.strftime('%Y-%m-%d') if partition_date else 'unknown'
                
                # Export data for this date
                parquet_key = f"{self.output_path}date={date_str}/deduplicated_{int(time.time())}.parquet"
                
                try:
                    self.conn.execute(f"""
                        COPY (
                            SELECT * FROM deduplicated_transactions 
                            WHERE partition_date = '{partition_date}'
                        ) TO 's3://{self.bucket}/{parquet_key}' 
                        (FORMAT PARQUET, COMPRESSION 'snappy')
                    """)
                    
                    # Count records for this date
                    date_count = self.conn.execute(f"""
                        SELECT COUNT(*) FROM deduplicated_transactions 
                        WHERE partition_date = '{partition_date}'
                    """).fetchone()[0]
                    
                    total_exported += date_count
                    logger.info(f"Exported {date_count:,} records for {date_str}")
                    
                except Exception as e:
                    logger.error(f"Failed to export date {date_str}: {e}")
                    continue
            
            return total_exported
            
        except Exception as e:
            logger.error(f"Export failed: {e}")
            return 0
    
    def validate_deduplication(self):
        """Validate the deduplicated dataset"""
        try:
            print(f"\n‚úÖ VALIDATION:")
            
            # Check for remaining duplicates
            remaining_dupes = self.conn.execute(f"""
                SELECT COUNT(*) as duplicate_pairs
                FROM (
                    SELECT wallet_address, transaction_hash, COUNT(*) as cnt
                    FROM read_parquet('s3://{self.bucket}/{self.output_path}**/*.parquet')
                    GROUP BY wallet_address, transaction_hash
                    HAVING COUNT(*) > 1
                )
            """).fetchone()[0]
            
            # Get final statistics
            final_stats = self.conn.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT transaction_hash) as unique_transactions,
                    MIN(timestamp) as earliest,
                    MAX(timestamp) as latest
                FROM read_parquet('s3://{self.bucket}/{self.output_path}**/*.parquet')
            """).fetchone()
            
            print(f"  Remaining Duplicates: {remaining_dupes:,}")
            print(f"  Final Record Count: {final_stats[0]:,}")
            print(f"  Unique Wallets: {final_stats[1]:,}")
            print(f"  Unique Transactions: {final_stats[2]:,}")
            print(f"  Date Range: {final_stats[3]} to {final_stats[4]}")
            
            success = remaining_dupes == 0
            print(f"  Status: {'‚úÖ CLEAN' if success else '‚ùå DUPLICATES REMAIN'}")
            
            return success
            
        except Exception as e:
            logger.error(f"Validation failed: {e}")
            return False
    
    def run_deduplication(self):
        """Execute complete deduplication process"""
        start_time = time.time()
        
        print("\n" + "="*70)
        print("WALLET TRANSACTION DEDUPLICATION")
        print("Processing Complete Migrated Dataset")
        print("="*70)
        
        # Step 1: Analyze duplicates
        has_duplicates = self.analyze_duplicates()
        
        if not has_duplicates:
            print(f"\nüéâ No deduplication needed - dataset is already clean!")
            return True
        
        # Step 2: Deduplicate data
        print(f"\nüöÄ STARTING DEDUPLICATION...")
        dedup_count = self.deduplicate_data()
        
        if dedup_count is None:
            print(f"‚ùå Deduplication failed")
            return False
        
        # Step 3: Export deduplicated data
        exported_count = self.export_deduplicated_data()
        
        if exported_count == 0:
            print(f"‚ùå Export failed")
            return False
        
        # Step 4: Validate results
        validation_success = self.validate_deduplication()
        
        # Final summary
        total_time = time.time() - start_time
        
        print(f"\nüéâ DEDUPLICATION COMPLETED!")
        print(f"üìä FINAL SUMMARY:")
        print(f"  Deduplicated Records: {dedup_count:,}")
        print(f"  Exported Records: {exported_count:,}")
        print(f"  Processing Time: {total_time/60:.1f} minutes")
        print(f"  Validation: {'‚úÖ PASSED' if validation_success else '‚ùå FAILED'}")
        print(f"  Output Location: s3://{self.bucket}/{self.output_path}")
        
        return validation_success
    
    def __del__(self):
        """Clean up connections"""
        if hasattr(self, 'conn'):
            self.conn.close()

if __name__ == "__main__":
    deduplicator = WalletTransactionDeduplicator()
    success = deduplicator.run_deduplication()
    exit(0 if success else 1)