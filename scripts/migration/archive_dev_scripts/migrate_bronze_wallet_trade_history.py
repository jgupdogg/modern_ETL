#!/usr/bin/env python3
"""
Large-Scale PostgreSQL to MinIO Migration: Bronze Wallet Trade History

Migrates 3.6M+ records from solana_pipeline.bronze.wallet_trade_history 
to MinIO with optimized batch processing and memory management.

Features:
- Batch processing (50K records per batch)
- Memory-efficient streaming
- Progress tracking and resumability
- USD value improvement for NULL records
- Date-based partitioning
- Error handling and retry logic

Author: Generated by Claude Code
Date: 2025-06-19
"""

import os
import logging
import json
import time
from typing import Dict, Optional, List
from datetime import datetime, timedelta
import pandas as pd
import psycopg2
from psycopg2.extras import DictCursor
import boto3
from botocore.exceptions import ClientError
import pyarrow as pa
import pyarrow.parquet as pq
import io

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('bronze_migration.log')
    ]
)
logger = logging.getLogger(__name__)

class BronzeWalletTradeHistoryMigrator:
    """Large-scale migration with optimization for 3.6M+ records"""
    
    def __init__(self):
        """Initialize for large-scale migration"""
        
        # PostgreSQL connection (solana_pipeline database)
        self.pg_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': 'solana_pipeline',
            'user': os.getenv('DB_USER', 'postgres'),
            'password': os.getenv('DB_PASSWORD', 'St0ck!adePG')
        }
        
        # MinIO connection
        self.s3_client = boto3.client(
            's3',
            endpoint_url=os.getenv('MINIO_ENDPOINT', 'http://localhost:9000'),
            aws_access_key_id=os.getenv('MINIO_ACCESS_KEY', 'minioadmin'),
            aws_secret_access_key=os.getenv('MINIO_SECRET_KEY', 'minioadmin123'),
            region_name='us-east-1'
        )
        
        # Configuration for large dataset
        self.source_table = 'bronze.wallet_trade_history'
        self.target_bucket = 'solana-data'
        self.target_path = 'bronze/wallet_trade_history_bronze/'
        self.batch_size = 50000  # 50K records per batch for memory efficiency
        self.status_file = 'bronze_migration_status.json'
        
        # Load migration status
        self.status = self.load_status()
        
    def load_status(self) -> Dict:
        """Load migration progress from status file"""
        if os.path.exists(self.status_file):
            try:
                with open(self.status_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Could not load status file: {e}")
        
        return {
            'completed_batches': 0,
            'total_rows': 0,
            'last_id': 0,
            'start_time': None,
            'status': 'pending'
        }
    
    def save_status(self):
        """Save migration progress"""
        try:
            with open(self.status_file, 'w') as f:
                json.dump(self.status, f, indent=2, default=str)
        except Exception as e:
            logger.error(f"Could not save status: {e}")
    
    def get_table_info(self) -> Dict:
        """Get table metadata for planning"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                with conn.cursor() as cur:
                    # Get total count
                    cur.execute(f"SELECT COUNT(*) FROM {self.source_table};")
                    total_count = cur.fetchone()[0]
                    
                    # Get ID range
                    cur.execute(f"SELECT MIN(id), MAX(id) FROM {self.source_table};")
                    min_id, max_id = cur.fetchone()
                    
                    # Get date range
                    cur.execute(f"SELECT MIN(timestamp), MAX(timestamp) FROM {self.source_table} WHERE timestamp IS NOT NULL;")
                    min_date, max_date = cur.fetchone()
                    
                    return {
                        'total_count': total_count,
                        'min_id': min_id,
                        'max_id': max_id,
                        'min_date': min_date,
                        'max_date': max_date,
                        'estimated_batches': (total_count + self.batch_size - 1) // self.batch_size
                    }
        except Exception as e:
            logger.error(f"Failed to get table info: {e}")
            return {}
    
    def extract_batch(self, start_id: int, batch_size: int) -> Optional[pd.DataFrame]:
        """Extract a batch of data with optimized query"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                # Optimized query with field mapping and USD calculation improvement
                query = f"""
                SELECT 
                    -- Core transaction identifiers
                    id,
                    wallet_address,
                    transaction_hash,
                    COALESCE(timestamp, created_at) as timestamp,
                    COALESCE(block_unix_time, 0) as block_slot,
                    
                    -- Unified token fields (map from_symbol/to_symbol to token_a/token_b)
                    from_symbol as token_a,
                    to_symbol as token_b,
                    COALESCE(from_amount, 0) as amount_a,
                    COALESCE(to_amount, 0) as amount_b,
                    
                    -- IMPROVED USD VALUE CALCULATION for NULL records
                    CASE 
                        WHEN value_usd IS NOT NULL AND value_usd > 0 THEN value_usd
                        WHEN base_price > 0 AND from_amount > 0 THEN from_amount * base_price
                        WHEN quote_price > 0 AND to_amount > 0 THEN to_amount * quote_price
                        ELSE 0
                    END as value_usd,
                    
                    -- Preserve price fields for PnL processing
                    COALESCE(base_price, 0) as base_price,
                    COALESCE(quote_price, 0) as quote_price,
                    0 as nearest_price,  -- Not available in bronze schema
                    
                    -- Transaction metadata
                    COALESCE(transaction_type, 'UNKNOWN') as transaction_type,
                    0.0 as transaction_fee,  -- Not available
                    true as success,  -- Assume success if in table
                    '' as error_message,
                    
                    -- Processing metadata
                    COALESCE(processed_at, created_at, CURRENT_TIMESTAMP) as processed_at,
                    CONCAT('bronze_batch_', %s) as batch_id,
                    COALESCE(processed_for_pnl, false) as processed_for_pnl,
                    NULL as pnl_processed_at,
                    COALESCE(processing_status, '') as pnl_processing_batch_id,
                    
                    -- Token details for PnL processing
                    from_address as token_a_address,
                    to_address as token_b_address,
                    COALESCE(from_decimals, 9) as token_a_decimals,
                    COALESCE(to_decimals, 6) as token_b_decimals,
                    
                    -- Source metadata
                    'bronze_wallet_trade_history' as schema_version,
                    'solana_pipeline_bronze' as migration_source,
                    CURRENT_TIMESTAMP as migration_timestamp,
                    DATE(COALESCE(timestamp, created_at)) as partition_date,
                    
                    -- Additional bronze fields
                    token_address,
                    source,
                    tx_type
                    
                FROM {self.source_table}
                WHERE id > %s AND id <= %s
                ORDER BY id;
                """
                
                end_id = start_id + batch_size
                df = pd.read_sql_query(query, conn, params=[start_id, start_id, end_id])
                
                if not df.empty:
                    logger.info(f"Extracted batch: IDs {start_id+1}-{min(end_id, df['id'].max())} ({len(df):,} records)")
                
                return df if not df.empty else None
                
        except Exception as e:
            logger.error(f"Failed to extract batch starting at ID {start_id}: {e}")
            return None
    
    def upload_batch_to_minio(self, df: pd.DataFrame, batch_num: int) -> bool:
        """Upload batch to MinIO with date partitioning"""
        try:
            # Group by partition_date for organized storage
            for date, date_df in df.groupby('partition_date'):
                if len(date_df) == 0:
                    continue
                
                # Convert to PyArrow table
                table = pa.Table.from_pandas(date_df.drop(['id'], axis=1))  # Remove ID for storage
                
                # Create parquet buffer
                buffer = io.BytesIO()
                pq.write_table(table, buffer, compression='snappy')
                buffer.seek(0)
                
                # Upload with date partition structure
                date_str = date.strftime('%Y-%m-%d') if date else 'unknown'
                timestamp = datetime.now().strftime('%H%M%S')
                key = f"{self.target_path}date={date_str}/batch_{batch_num:06d}_{timestamp}.parquet"
                
                self.s3_client.put_object(
                    Bucket=self.target_bucket,
                    Key=key,
                    Body=buffer.getvalue(),
                    ContentType='application/octet-stream'
                )
                
                logger.info(f"Uploaded {len(date_df):,} records to {key}")
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to upload batch {batch_num}: {e}")
            return False
    
    def run_migration(self):
        """Execute the large-scale migration with progress tracking"""
        
        print("\n" + "="*70)
        print("LARGE-SCALE BRONZE WALLET TRADE HISTORY MIGRATION")
        print("3.6M+ Records | PostgreSQL ‚Üí MinIO")
        print("="*70)
        
        # Get table information
        table_info = self.get_table_info()
        if not table_info:
            print("‚ùå Could not get table information")
            return False
        
        # Display migration plan
        total_count = table_info['total_count']
        estimated_batches = table_info['estimated_batches']
        
        print(f"\nüìä MIGRATION PLAN:")
        print(f"  Total Records: {total_count:,}")
        print(f"  Batch Size: {self.batch_size:,}")
        print(f"  Estimated Batches: {estimated_batches:,}")
        print(f"  ID Range: {table_info['min_id']:,} - {table_info['max_id']:,}")
        print(f"  Date Range: {table_info['min_date']} to {table_info['max_date']}")
        print(f"  Estimated Size: ~{(total_count * 2000 / 1024**3):.1f} GB")
        
        # Check if resuming
        if self.status['completed_batches'] > 0:
            print(f"\nüîÑ RESUMING MIGRATION:")
            print(f"  Completed Batches: {self.status['completed_batches']:,}")
            print(f"  Last ID: {self.status['last_id']:,}")
            print(f"  Progress: {self.status['completed_batches'] / estimated_batches * 100:.1f}%")
        
        # Setup MinIO bucket
        try:
            self.s3_client.head_bucket(Bucket=self.target_bucket)
            logger.info(f"‚úÖ MinIO bucket '{self.target_bucket}' ready")
        except ClientError:
            try:
                self.s3_client.create_bucket(Bucket=self.target_bucket)
                logger.info(f"‚úÖ Created MinIO bucket '{self.target_bucket}'")
            except Exception as e:
                print(f"‚ùå MinIO setup failed: {e}")
                return False
        
        # Initialize status
        if self.status['status'] == 'pending':
            self.status.update({
                'total_rows': total_count,
                'start_time': datetime.now(),
                'status': 'running'
            })
            self.save_status()
        
        # Process batches
        start_id = self.status['last_id']
        batch_num = self.status['completed_batches']
        successful_batches = self.status.get('successful_batches', 0)
        
        print(f"\nüöÄ STARTING BATCH PROCESSING...")
        start_time = time.time()
        
        while start_id < table_info['max_id']:
            batch_start_time = time.time()
            
            # Extract batch
            df = self.extract_batch(start_id, self.batch_size)
            if df is None or df.empty:
                logger.warning(f"No data in batch starting at ID {start_id}")
                start_id += self.batch_size
                continue
            
            # Upload batch
            if not self.upload_batch_to_minio(df, batch_num):
                print(f"‚ùå Failed to upload batch {batch_num}")
                return False
            
            # Update progress - FIXED: Continue processing until all records done
            actual_max_id = df['id'].max()
            start_id = actual_max_id  # Continue from the last processed ID
            batch_num += 1
            successful_batches += 1
            
            # Update status
            self.status.update({
                'completed_batches': batch_num,
                'last_id': start_id,
                'successful_batches': successful_batches
            })
            self.save_status()
            
            # Progress reporting
            batch_time = time.time() - batch_start_time
            total_time = time.time() - start_time
            progress = batch_num / estimated_batches * 100
            
            records_processed = successful_batches * self.batch_size
            records_per_sec = records_processed / total_time if total_time > 0 else 0
            eta_seconds = (total_count - records_processed) / records_per_sec if records_per_sec > 0 else 0
            eta = str(timedelta(seconds=int(eta_seconds)))
            
            print(f"  ‚úÖ Batch {batch_num:,}/{estimated_batches:,} | "
                  f"{progress:.1f}% | "
                  f"{len(df):,} records | "
                  f"{batch_time:.1f}s | "
                  f"ETA: {eta}")
            
            # Memory cleanup
            del df
            
            # Brief pause to prevent overwhelming the systems
            time.sleep(0.1)
        
        # Mark as completed
        self.status['status'] = 'completed'
        self.status['end_time'] = datetime.now()
        self.save_status()
        
        # Final statistics
        total_time = time.time() - start_time
        total_minutes = total_time / 60
        
        print(f"\n‚úÖ MIGRATION COMPLETED SUCCESSFULLY!")
        print(f"üìä FINAL STATISTICS:")
        print(f"  Total Batches: {successful_batches:,}")
        print(f"  Total Time: {total_minutes:.1f} minutes")
        print(f"  Records/Second: {records_processed / total_time:.0f}")
        print(f"  Records/Minute: {records_processed / total_minutes:.0f}")
        
        print(f"\nüìÇ OUTPUT LOCATION:")
        print(f"  s3://{self.target_bucket}/{self.target_path}")
        print(f"  Date-partitioned Parquet files")
        
        print(f"\nüéØ READY FOR INTEGRATION:")
        print(f"  Compatible with existing wallet transaction analysis")
        print(f"  USD values improved for NULL records")
        print(f"  Optimized for PnL calculations")
        
        return True

if __name__ == "__main__":
    migrator = BronzeWalletTradeHistoryMigrator()
    success = migrator.run_migration()
    exit(0 if success else 1)