#!/usr/bin/env python3
"""
Bronze Wallet Transactions DuckDB-Based Migration Script

This script uses DuckDB to harmonize old and current bronze wallet transaction schemas
and create a unified dataset for comprehensive smart trader analysis.

Based on actual schema analysis:
- Old schema: 8,102 records, 163 wallets (from_symbol/to_symbol format)
- Current schema: 79,605 records, 999 wallets (also from_symbol/to_symbol format)

Author: Generated by Claude Code
Date: 2025-06-19
"""

import duckdb
import json
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DuckDBWalletTransactionsMigrator:
    """DuckDB-based migrator for bronze wallet transaction schemas"""
    
    def __init__(self):
        """Initialize the DuckDB-based migrator"""
        
        self.old_schema_path = "s3://solana-data/bronze/wallet_transactions_old_schema/**/*.parquet"
        self.current_schema_path = "s3://solana-data/bronze/wallet_transactions/**/wallet_transactions_*.parquet" 
        self.unified_output_path = "s3://solana-data/bronze/wallet_transactions_unified/"
        
        # Initialize DuckDB connection
        self.conn = duckdb.connect()
        
        # Configure MinIO/S3 settings
        self.conn.execute("""
            INSTALL httpfs;
            LOAD httpfs;
            SET s3_endpoint='localhost:9000';
            SET s3_access_key_id='minioadmin';
            SET s3_secret_access_key='minioadmin123';
            SET s3_use_ssl=false;
            SET s3_url_style='path';
        """)
        
        logger.info("DuckDB migrator initialized with MinIO configuration")

    def analyze_schemas(self) -> Dict[str, any]:
        """Analyze both schemas to understand the data structure"""
        analysis = {
            'old_schema': {},
            'current_schema': {},
            'comparison': {}
        }
        
        try:
            # Analyze old schema
            logger.info("Analyzing old schema...")
            old_stats = self.conn.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT token_address) as unique_tokens,
                    MIN(timestamp) as earliest_transaction,
                    MAX(timestamp) as latest_transaction,
                    COUNT(DISTINCT from_symbol) as unique_from_symbols,
                    COUNT(DISTINCT to_symbol) as unique_to_symbols,
                    AVG(value_usd) as avg_usd_value
                FROM read_parquet('{self.old_schema_path}')
            """).fetchone()
            
            old_columns = self.conn.execute(f"""
                DESCRIBE SELECT * FROM read_parquet('{self.old_schema_path}') LIMIT 1
            """).fetchall()
            
            analysis['old_schema'] = {
                'total_records': old_stats[0],
                'unique_wallets': old_stats[1],
                'unique_tokens': old_stats[2],
                'time_range': f"{old_stats[3]} to {old_stats[4]}",
                'unique_from_symbols': old_stats[5],
                'unique_to_symbols': old_stats[6],
                'avg_usd_value': round(old_stats[7], 4) if old_stats[7] else 0,
                'columns': [col[0] for col in old_columns],
                'column_count': len(old_columns)
            }
            
            # Analyze current schema
            logger.info("Analyzing current schema...")
            current_stats = self.conn.execute(f"""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT token_address) as unique_tokens,
                    MIN(timestamp) as earliest_transaction,
                    MAX(timestamp) as latest_transaction,
                    COUNT(DISTINCT from_symbol) as unique_from_symbols,
                    COUNT(DISTINCT to_symbol) as unique_to_symbols,
                    AVG(value_usd) as avg_usd_value
                FROM read_parquet('{self.current_schema_path}')
            """).fetchone()
            
            current_columns = self.conn.execute(f"""
                DESCRIBE SELECT * FROM read_parquet('{self.current_schema_path}') LIMIT 1
            """).fetchall()
            
            analysis['current_schema'] = {
                'total_records': current_stats[0],
                'unique_wallets': current_stats[1],
                'unique_tokens': current_stats[2],
                'time_range': f"{current_stats[3]} to {current_stats[4]}",
                'unique_from_symbols': current_stats[5],
                'unique_to_symbols': current_stats[6],
                'avg_usd_value': round(current_stats[7], 4) if current_stats[7] else 0,
                'columns': [col[0] for col in current_columns],
                'column_count': len(current_columns)
            }
            
            # Schema comparison
            old_cols = set(analysis['old_schema']['columns'])
            current_cols = set(analysis['current_schema']['columns'])
            
            analysis['comparison'] = {
                'common_columns': list(old_cols & current_cols),
                'old_only_columns': list(old_cols - current_cols),
                'current_only_columns': list(current_cols - old_cols),
                'schema_identical': old_cols == current_cols
            }
            
            logger.info(f"Schema analysis complete:")
            logger.info(f"  Old: {analysis['old_schema']['total_records']} records, {analysis['old_schema']['unique_wallets']} wallets")
            logger.info(f"  Current: {analysis['current_schema']['total_records']} records, {analysis['current_schema']['unique_wallets']} wallets")
            logger.info(f"  Schemas identical: {analysis['comparison']['schema_identical']}")
            
        except Exception as e:
            logger.error(f"Error analyzing schemas: {e}")
            analysis['error'] = str(e)
        
        return analysis

    def create_unified_dataset(self) -> Dict[str, any]:
        """Create unified dataset by combining both schemas"""
        result = {
            'success': False,
            'unified_records': 0,
            'unified_wallets': 0,
            'output_files': [],
            'errors': []
        }
        
        try:
            logger.info("Creating unified dataset...")
            
            # Check if schemas are compatible
            analysis = self.analyze_schemas()
            if 'error' in analysis:
                result['errors'].append(f"Schema analysis failed: {analysis['error']}")
                return result
            
            # Create unified view with standardized columns
            unified_query = f"""
            WITH old_data AS (
                SELECT 
                    wallet_address,
                    token_address,
                    transaction_hash,
                    timestamp,
                    block_slot,
                    from_symbol,
                    to_symbol,
                    from_amount,
                    to_amount,
                    transaction_type,
                    value_usd,
                    nearest_price,
                    transaction_fee,
                    success,
                    error_message,
                    processed_at,
                    batch_id,
                    COALESCE(processed_for_pnl, false) as processed_for_pnl,
                    pnl_processed_at,
                    pnl_processing_batch_id,
                    'old_schema' as migration_source,
                    CAST('2025-06-19T23:00:00Z' as TIMESTAMP) as migration_timestamp,
                    DATE(timestamp) as partition_date
                FROM read_parquet('{self.old_schema_path}')
            ),
            current_data AS (
                SELECT 
                    wallet_address,
                    token_address,
                    transaction_hash,
                    timestamp,
                    block_slot,
                    from_symbol,
                    to_symbol,
                    from_amount,
                    to_amount,
                    transaction_type,
                    value_usd,
                    nearest_price,
                    transaction_fee,
                    success,
                    error_message,
                    processed_at,
                    batch_id,
                    COALESCE(processed_for_pnl, false) as processed_for_pnl,
                    pnl_processed_at,
                    pnl_processing_batch_id,
                    'current_schema' as migration_source,
                    CAST('2025-06-19T23:00:00Z' as TIMESTAMP) as migration_timestamp,
                    DATE(timestamp) as partition_date
                FROM read_parquet('{self.current_schema_path}')
            ),
            unified AS (
                SELECT * FROM old_data
                UNION ALL
                SELECT * FROM current_data
            )
            SELECT 
                *,
                ROW_NUMBER() OVER (PARTITION BY wallet_address, transaction_hash ORDER BY timestamp) as dedup_rank
            FROM unified
            """
            
            # Execute unified query and get statistics
            logger.info("Executing unified dataset query...")
            
            # Get unified statistics
            unified_stats = self.conn.execute(f"""
                WITH unified_data AS ({unified_query})
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(CASE WHEN migration_source = 'old_schema' THEN 1 END) as old_records,
                    COUNT(CASE WHEN migration_source = 'current_schema' THEN 1 END) as current_records,
                    COUNT(CASE WHEN dedup_rank = 1 THEN 1 END) as unique_transactions,
                    MIN(timestamp) as earliest_transaction,
                    MAX(timestamp) as latest_transaction
                FROM unified_data
            """).fetchone()
            
            result['unified_records'] = unified_stats[0]
            result['unified_wallets'] = unified_stats[1]
            result['old_records'] = unified_stats[2]
            result['current_records'] = unified_stats[3]
            result['unique_transactions'] = unified_stats[4]
            result['time_range'] = f"{unified_stats[5]} to {unified_stats[6]}"
            
            # Write unified dataset to parquet files (partitioned by date)
            logger.info("Writing unified dataset to MinIO...")
            
            # Create partitioned output
            partition_query = f"""
            COPY (
                WITH unified_data AS ({unified_query})
                SELECT * FROM unified_data
                WHERE dedup_rank = 1  -- Remove duplicates
                ORDER BY timestamp, wallet_address
            ) TO '{self.unified_output_path}' (
                FORMAT PARQUET,
                PARTITION_BY partition_date,
                OVERWRITE_OR_IGNORE true
            )
            """
            
            self.conn.execute(partition_query)
            
            # Write summary metadata
            summary_data = {
                'migration_timestamp': datetime.now(timezone.utc).isoformat(),
                'source_analysis': analysis,
                'unified_statistics': {
                    'total_records': result['unified_records'],
                    'unique_wallets': result['unified_wallets'],
                    'unique_transactions': result['unique_transactions'],
                    'old_schema_records': result['old_records'],
                    'current_schema_records': result['current_records'],
                    'time_range': result['time_range']
                },
                'migration_notes': {
                    'deduplication_applied': True,
                    'partitioned_by_date': True,
                    'schemas_compatible': analysis['comparison']['schema_identical'],
                    'ready_for_analysis': True
                }
            }
            
            # Write summary as JSON
            summary_json = json.dumps(summary_data, indent=2)
            self.conn.execute(f"""
                COPY (SELECT '{summary_json}' as summary) 
                TO '{self.unified_output_path}migration_summary.json' 
                (FORMAT JSON)
            """)
            
            result['success'] = True
            result['output_files'].append(f"{self.unified_output_path}*/*.parquet")
            result['output_files'].append(f"{self.unified_output_path}migration_summary.json")
            
            logger.info(f"Unified dataset created successfully:")
            logger.info(f"  Total records: {result['unified_records']:,}")
            logger.info(f"  Unique wallets: {result['unified_wallets']:,}")
            logger.info(f"  Unique transactions: {result['unique_transactions']:,}")
            logger.info(f"  Output location: {self.unified_output_path}")
            
        except Exception as e:
            error_msg = f"Error creating unified dataset: {e}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
        
        return result

    def validate_unified_dataset(self) -> Dict[str, any]:
        """Validate the unified dataset"""
        validation = {
            'success': False,
            'checks': {},
            'errors': []
        }
        
        try:
            logger.info("Validating unified dataset...")
            
            # Check if unified data exists
            try:
                unified_stats = self.conn.execute(f"""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(DISTINCT wallet_address) as unique_wallets,
                        COUNT(DISTINCT transaction_hash) as unique_transactions,
                        COUNT(DISTINCT DATE(timestamp)) as unique_dates,
                        MIN(timestamp) as earliest,
                        MAX(timestamp) as latest,
                        COUNT(CASE WHEN migration_source = 'old_schema' THEN 1 END) as old_records,
                        COUNT(CASE WHEN migration_source = 'current_schema' THEN 1 END) as current_records
                    FROM read_parquet('{self.unified_output_path}**/*.parquet')
                """).fetchone()
                
                validation['checks']['data_exists'] = True
                validation['checks']['total_records'] = unified_stats[0]
                validation['checks']['unique_wallets'] = unified_stats[1]
                validation['checks']['unique_transactions'] = unified_stats[2]
                validation['checks']['date_range_days'] = unified_stats[3]
                validation['checks']['time_span'] = f"{unified_stats[4]} to {unified_stats[5]}"
                validation['checks']['old_schema_records'] = unified_stats[6]
                validation['checks']['current_schema_records'] = unified_stats[7]
                
            except Exception as e:
                validation['checks']['data_exists'] = False
                validation['errors'].append(f"Cannot access unified data: {e}")
                return validation
            
            # Data quality checks
            quality_checks = self.conn.execute(f"""
                SELECT 
                    COUNT(CASE WHEN wallet_address IS NULL OR wallet_address = '' THEN 1 END) as null_wallets,
                    COUNT(CASE WHEN transaction_hash IS NULL OR transaction_hash = '' THEN 1 END) as null_hashes,
                    COUNT(CASE WHEN timestamp IS NULL THEN 1 END) as null_timestamps,
                    COUNT(CASE WHEN value_usd IS NULL OR value_usd <= 0 THEN 1 END) as null_usd_values,
                    COUNT(CASE WHEN success = false THEN 1 END) as failed_transactions,
                    AVG(value_usd) as avg_usd_value,
                    MAX(value_usd) as max_usd_value
                FROM read_parquet('{self.unified_output_path}**/*.parquet')
            """).fetchone()
            
            validation['checks']['quality'] = {
                'null_wallets': quality_checks[0],
                'null_transaction_hashes': quality_checks[1],
                'null_timestamps': quality_checks[2],
                'null_usd_values': quality_checks[3],
                'failed_transactions': quality_checks[4],
                'avg_usd_value': round(quality_checks[5], 4) if quality_checks[5] else 0,
                'max_usd_value': round(quality_checks[6], 4) if quality_checks[6] else 0
            }
            
            # Sample data verification
            sample_data = self.conn.execute(f"""
                SELECT 
                    wallet_address,
                    from_symbol,
                    to_symbol,
                    value_usd,
                    migration_source,
                    partition_date
                FROM read_parquet('{self.unified_output_path}**/*.parquet')
                ORDER BY value_usd DESC
                LIMIT 5
            """).fetchall()
            
            validation['checks']['sample_data'] = [
                {
                    'wallet': row[0][:8] + '...',  # Truncate for privacy
                    'trade': f"{row[1]} -> {row[2]}",
                    'value_usd': row[3],
                    'source': row[4],
                    'date': str(row[5])
                } for row in sample_data
            ]
            
            validation['success'] = True
            logger.info("Unified dataset validation completed successfully")
            
        except Exception as e:
            error_msg = f"Validation error: {e}"
            logger.error(error_msg)
            validation['errors'].append(error_msg)
        
        return validation

    def run_complete_migration(self) -> Dict[str, any]:
        """Run the complete migration process"""
        logger.info("Starting complete bronze wallet transactions migration")
        
        migration_result = {
            'start_time': datetime.now(timezone.utc).isoformat(),
            'schema_analysis': {},
            'unified_dataset': {},
            'validation': {},
            'success': False,
            'errors': []
        }
        
        try:
            # Step 1: Analyze schemas
            migration_result['schema_analysis'] = self.analyze_schemas()
            
            # Step 2: Create unified dataset
            migration_result['unified_dataset'] = self.create_unified_dataset()
            
            # Step 3: Validate unified dataset
            if migration_result['unified_dataset']['success']:
                migration_result['validation'] = self.validate_unified_dataset()
                migration_result['success'] = migration_result['validation']['success']
            
            migration_result['end_time'] = datetime.now(timezone.utc).isoformat()
            
        except Exception as e:
            error_msg = f"Migration process failed: {e}"
            logger.error(error_msg)
            migration_result['errors'].append(error_msg)
        
        finally:
            self.conn.close()
        
        return migration_result


def main():
    """Main function to run the migration"""
    print("Bronze Wallet Transactions DuckDB Migration")
    print("=" * 50)
    
    # Initialize migrator
    migrator = DuckDBWalletTransactionsMigrator()
    
    # Run complete migration
    results = migrator.run_complete_migration()
    
    # Print results
    print(f"\nMigration Results:")
    print(f"Success: {results['success']}")
    
    if 'schema_analysis' in results and 'old_schema' in results['schema_analysis']:
        old = results['schema_analysis']['old_schema']
        current = results['schema_analysis']['current_schema']
        print(f"\nSource Data:")
        print(f"  Old Schema: {old.get('total_records', 0):,} records, {old.get('unique_wallets', 0):,} wallets")
        print(f"  Current Schema: {current.get('total_records', 0):,} records, {current.get('unique_wallets', 0):,} wallets")
    
    if 'unified_dataset' in results and results['unified_dataset'].get('success'):
        unified = results['unified_dataset']
        print(f"\nUnified Dataset:")
        print(f"  Total Records: {unified.get('unified_records', 0):,}")
        print(f"  Unique Wallets: {unified.get('unified_wallets', 0):,}")
        print(f"  Unique Transactions: {unified.get('unique_transactions', 0):,}")
        print(f"  Time Range: {unified.get('time_range', 'Unknown')}")
    
    if 'validation' in results and results['validation'].get('success'):
        validation = results['validation']['checks']
        print(f"\nData Quality:")
        print(f"  Quality Score: Good ({validation['quality']['failed_transactions']} failed transactions)")
        print(f"  Avg Transaction Value: ${validation['quality']['avg_usd_value']}")
        print(f"  Date Range: {validation['date_range_days']} unique days")
    
    if results.get('errors'):
        print(f"\nErrors:")
        for error in results['errors']:
            print(f"  - {error}")
    
    # Exit with appropriate code
    sys.exit(0 if results['success'] else 1)


if __name__ == "__main__":
    main()