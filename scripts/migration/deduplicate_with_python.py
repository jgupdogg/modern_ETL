#!/usr/bin/env python3
"""
Python-Based Deduplication Script

Uses boto3 and pandas to deduplicate wallet transactions.
Processes data in chunks to handle the 3.6M record dataset.

Strategy:
1. Read all parquet files in chunks
2. Collect all records with their timestamps
3. Group by (wallet_address, transaction_hash)
4. Keep only the record with the latest timestamp
5. Write clean data back to MinIO

Author: Generated by Claude Code
Date: 2025-06-19
"""

import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import io
import time
import logging
from datetime import datetime
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PythonDeduplicator:
    """Python-based deduplication using boto3 and pandas"""
    
    def __init__(self):
        """Initialize MinIO connection"""
        self.s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin123',
            region_name='us-east-1'
        )
        
        self.bucket = 'solana-data'
        self.source_prefix = 'bronze/wallet_trade_history_bronze_fixed/'
        self.output_prefix = 'bronze/wallet_transactions_deduplicated/'
        
    def get_all_files(self):
        """Get list of all parquet files"""
        files = []
        paginator = self.s3_client.get_paginator('list_objects_v2')
        
        for page in paginator.paginate(Bucket=self.bucket, Prefix=self.source_prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    if obj['Key'].endswith('.parquet'):
                        files.append(obj['Key'])
        
        return files
    
    def read_parquet_file(self, key):
        """Read a single parquet file from MinIO"""
        try:
            response = self.s3_client.get_object(Bucket=self.bucket, Key=key)
            parquet_buffer = io.BytesIO(response['Body'].read())
            table = pq.read_table(parquet_buffer)
            df = table.to_pandas()
            return df
        except Exception as e:
            logger.error(f"Failed to read {key}: {e}")
            return None
    
    def process_files_in_batches(self, files, batch_size=100):
        """Process files in batches to manage memory"""
        all_records = []
        
        for i in range(0, len(files), batch_size):
            batch_files = files[i:i+batch_size]
            batch_records = []
            
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(files) + batch_size - 1)//batch_size}: {len(batch_files)} files")
            
            for file_key in batch_files:
                df = self.read_parquet_file(file_key)
                if df is not None and not df.empty:
                    # Convert to records for easier processing
                    for _, row in df.iterrows():
                        record = {
                            'wallet_address': row['wallet_address'],
                            'transaction_hash': row['transaction_hash'], 
                            'timestamp': row['timestamp'],
                            'data': row.to_dict()  # Store full record
                        }
                        batch_records.append(record)
            
            all_records.extend(batch_records)
            logger.info(f"Batch {i//batch_size + 1} processed: {len(batch_records):,} records")
            
            # Memory cleanup
            del batch_records
        
        return all_records
    
    def deduplicate_records(self, all_records):
        """Deduplicate records keeping the most recent timestamp"""
        logger.info(f"Deduplicating {len(all_records):,} records...")
        
        # Group by (wallet_address, transaction_hash)
        groups = defaultdict(list)
        
        for record in all_records:
            key = (record['wallet_address'], record['transaction_hash'])
            groups[key].append(record)
        
        # Keep only the most recent record for each group
        deduplicated = []
        duplicates_removed = 0
        
        for key, group in groups.items():
            if len(group) > 1:
                # Sort by timestamp and keep the latest
                group.sort(key=lambda x: x['timestamp'], reverse=True)
                duplicates_removed += len(group) - 1
            
            # Keep the first (most recent) record
            deduplicated.append(group[0]['data'])
        
        logger.info(f"Deduplication complete: {len(deduplicated):,} unique records, {duplicates_removed:,} duplicates removed")
        return deduplicated
    
    def write_clean_data(self, clean_records):
        """Write deduplicated data to MinIO with date partitioning"""
        logger.info(f"Writing {len(clean_records):,} clean records to MinIO...")
        
        # Convert to DataFrame
        df = pd.DataFrame(clean_records)
        
        # Group by partition_date for organized storage
        if 'partition_date' not in df.columns:
            # Create partition_date from timestamp if missing
            df['partition_date'] = pd.to_datetime(df['timestamp']).dt.date
        
        total_written = 0
        
        for date, date_df in df.groupby('partition_date'):
            if len(date_df) == 0:
                continue
            
            # Convert to PyArrow table
            table = pa.Table.from_pandas(date_df)
            
            # Create parquet buffer
            buffer = io.BytesIO()
            pq.write_table(table, buffer, compression='snappy')
            buffer.seek(0)
            
            # Upload with date partition structure
            date_str = date.strftime('%Y-%m-%d') if hasattr(date, 'strftime') else str(date)
            timestamp = int(time.time())
            key = f"{self.output_prefix}date={date_str}/clean_{timestamp}.parquet"
            
            try:
                self.s3_client.put_object(
                    Bucket=self.bucket,
                    Key=key,
                    Body=buffer.getvalue(),
                    ContentType='application/octet-stream'
                )
                
                total_written += len(date_df)
                logger.info(f"Written {len(date_df):,} records for {date_str}")
                
            except Exception as e:
                logger.error(f"Failed to write {key}: {e}")
                continue
        
        return total_written
    
    def run_deduplication(self):
        """Execute the complete deduplication process"""
        start_time = time.time()
        
        print("\n" + "="*70)
        print("PYTHON-BASED DEDUPLICATION")
        print("Processing 3.6M+ Records")
        print("="*70)
        
        # Step 1: Get all files
        print("\nüìÅ DISCOVERING FILES...")
        files = self.get_all_files()
        logger.info(f"Found {len(files):,} parquet files")
        
        if len(files) == 0:
            print("‚ùå No files found")
            return False
        
        # Step 2: Process files and collect records
        print(f"\nüìñ READING DATA...")
        all_records = self.process_files_in_batches(files, batch_size=50)
        
        if len(all_records) == 0:
            print("‚ùå No records found")
            return False
        
        # Step 3: Deduplicate
        print(f"\nüîÑ DEDUPLICATING...")
        clean_records = self.deduplicate_records(all_records)
        
        # Step 4: Write clean data
        print(f"\nüíæ WRITING CLEAN DATA...")
        written_count = self.write_clean_data(clean_records)
        
        # Final summary
        total_time = time.time() - start_time
        original_count = len(all_records)
        duplicates_removed = original_count - len(clean_records)
        
        print(f"\nüéâ DEDUPLICATION COMPLETED!")
        print(f"üìä SUMMARY:")
        print(f"  Original Records: {original_count:,}")
        print(f"  Clean Records: {len(clean_records):,}")
        print(f"  Duplicates Removed: {duplicates_removed:,}")
        print(f"  Deduplication Rate: {duplicates_removed/original_count*100:.2f}%")
        print(f"  Written Records: {written_count:,}")
        print(f"  Processing Time: {total_time/60:.1f} minutes")
        print(f"  Output: s3://{self.bucket}/{self.output_prefix}")
        
        return written_count == len(clean_records)

if __name__ == "__main__":
    deduplicator = PythonDeduplicator()
    success = deduplicator.run_deduplication()
    exit(0 if success else 1)