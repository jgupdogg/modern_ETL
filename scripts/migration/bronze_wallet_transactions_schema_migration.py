#!/usr/bin/env python3
"""
Bronze Wallet Transactions Schema Migration Script

This script harmonizes the old and current bronze wallet transaction schemas
to enable unified analysis across both datasets using DuckDB for robust parquet handling.

Author: Generated by Claude Code
Date: 2025-06-19
"""

import duckdb
import json
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BronzeWalletTransactionsMigrator:
    """Migrates and harmonizes bronze wallet transaction schemas"""
    
    def __init__(self, 
                 minio_endpoint: str = "http://localhost:9000",
                 access_key: str = "minioadmin", 
                 secret_key: str = "minioadmin123",
                 bucket_name: str = "solana-data"):
        """Initialize the migrator with MinIO configuration"""
        
        self.bucket_name = bucket_name
        self.old_schema_prefix = "bronze/wallet_transactions_old_schema/"
        self.current_schema_prefix = "bronze/wallet_transactions/"
        self.unified_schema_prefix = "bronze/wallet_transactions_unified/"
        
        # Initialize MinIO client
        self.s3_client = boto3.client(
            's3',
            endpoint_url=minio_endpoint,
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            config=Config(signature_version='s3v4')
        )
        
        # Schema mappings
        self.field_mappings = {
            # Direct mappings from old to new schema
            'wallet_address': 'wallet_address',
            'transaction_hash': 'transaction_hash', 
            'timestamp': 'timestamp',
            'block_slot': 'block_slot',
            'from_symbol': 'base_symbol',  # Key mapping
            'to_symbol': 'quote_symbol',   # Key mapping
            'from_amount': 'base_ui_amount',  # Key mapping
            'to_amount': 'quote_ui_amount',   # Key mapping
            'nearest_price': 'nearest_price',
            'transaction_fee': 'transaction_fee',
            'success': 'success',
            'error_message': 'error_message',
            'processed_at': 'processed_at',
            'batch_id': 'batch_id',
            'pnl_processed_at': 'pnl_processed_at',
            'pnl_processing_batch_id': 'pnl_processing_batch_id'
        }
        
        # Target unified schema (30 columns matching current schema)
        self.unified_schema = [
            'wallet_address', 'transaction_hash', 'timestamp', 'block_slot',
            'base_symbol', 'quote_symbol', 'base_ui_amount', 'quote_ui_amount',
            'base_price', 'quote_price', 'nearest_price', 'base_type_swap', 'quote_type_swap',
            'transaction_fee', 'success', 'error_message', 'owner',
            'processed_at', 'batch_id', 'processed_for_pnl', 'pnl_processed_at',
            'pnl_processing_batch_id', 'partition_date', 'migration_source',
            'migration_timestamp', 'transaction_type', 'value_usd', 'derived_fields',
            'data_quality_score', 'processing_version'
        ]

    def list_files(self, prefix: str) -> List[str]:
        """List all parquet files in a given S3 prefix"""
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix
            )
            
            files = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['Key'].endswith('.parquet'):
                        files.append(obj['Key'])
            
            logger.info(f"Found {len(files)} parquet files in {prefix}")
            return files
            
        except Exception as e:
            logger.error(f"Error listing files in {prefix}: {e}")
            return []

    def read_parquet_from_s3(self, s3_key: str) -> Optional[pd.DataFrame]:
        """Read a parquet file from S3"""
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            df = pd.read_parquet(response['Body'])
            logger.info(f"Read {len(df)} rows from {s3_key}")
            return df
        except Exception as e:
            logger.error(f"Error reading {s3_key}: {e}")
            return None

    def derive_transaction_type(self, row: pd.Series) -> str:
        """Derive transaction type from swap direction fields"""
        try:
            # Use base_type_swap and quote_type_swap if available
            if 'base_type_swap' in row and 'quote_type_swap' in row:
                base_type = str(row['base_type_swap']).lower()
                quote_type = str(row['quote_type_swap']).lower()
                
                # Determine if it's a buy or sell based on swap types
                if 'out' in base_type and 'in' in quote_type:
                    return 'SELL'  # Selling base token, getting quote token
                elif 'in' in base_type and 'out' in quote_type:
                    return 'BUY'   # Buying base token, selling quote token
            
            # Fallback logic using amount patterns
            base_amount = row.get('base_ui_amount', 0)
            quote_amount = row.get('quote_ui_amount', 0)
            
            if base_amount > 0 and quote_amount > 0:
                # Assume larger amount is what's being acquired
                return 'BUY' if base_amount > quote_amount else 'SELL'
            
            return 'UNKNOWN'
            
        except Exception as e:
            logger.warning(f"Error deriving transaction type: {e}")
            return 'UNKNOWN'

    def calculate_usd_value(self, row: pd.Series) -> float:
        """Calculate USD value from available price and amount data"""
        try:
            # Try to use nearest_price with base amount
            if 'nearest_price' in row and 'base_ui_amount' in row:
                price = row['nearest_price']
                amount = row['base_ui_amount']
                if pd.notna(price) and pd.notna(amount) and price > 0:
                    return float(price * amount)
            
            # Try base_price approach
            if 'base_price' in row and 'base_ui_amount' in row:
                price = row['base_price']
                amount = row['base_ui_amount']
                if pd.notna(price) and pd.notna(amount) and price > 0:
                    return float(price * amount)
            
            # Try quote_price approach
            if 'quote_price' in row and 'quote_ui_amount' in row:
                price = row['quote_price']
                amount = row['quote_ui_amount']
                if pd.notna(price) and pd.notna(amount) and price > 0:
                    return float(price * amount)
            
            return 0.0
            
        except Exception as e:
            logger.warning(f"Error calculating USD value: {e}")
            return 0.0

    def calculate_data_quality_score(self, row: pd.Series) -> float:
        """Calculate data quality score based on field completeness"""
        try:
            required_fields = ['wallet_address', 'transaction_hash', 'timestamp', 
                             'base_symbol', 'quote_symbol', 'base_ui_amount', 'quote_ui_amount']
            
            score = 0.0
            for field in required_fields:
                if field in row and pd.notna(row[field]) and row[field] != '':
                    score += 1.0
            
            # Bonus for price data
            if 'nearest_price' in row and pd.notna(row['nearest_price']) and row['nearest_price'] > 0:
                score += 0.5
            
            # Bonus for success flag
            if 'success' in row and row['success'] is True:
                score += 0.5
            
            return round(score / len(required_fields), 2)
            
        except Exception as e:
            logger.warning(f"Error calculating data quality score: {e}")
            return 0.0

    def transform_old_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform old schema DataFrame to unified schema"""
        try:
            # Start with mapped columns
            transformed = pd.DataFrame()
            
            # Apply direct field mappings
            for old_field, new_field in self.field_mappings.items():
                if old_field in df.columns:
                    transformed[new_field] = df[old_field]
            
            # Add missing columns with default values
            for col in self.unified_schema:
                if col not in transformed.columns:
                    if col == 'base_type_swap':
                        transformed[col] = 'out'  # Default assumption
                    elif col == 'quote_type_swap':
                        transformed[col] = 'in'   # Default assumption
                    elif col == 'owner':
                        transformed[col] = transformed.get('wallet_address', '')
                    elif col == 'processed_for_pnl':
                        transformed[col] = False  # Default to unprocessed
                    elif col == 'partition_date':
                        # Extract date from timestamp
                        if 'timestamp' in transformed.columns:
                            transformed[col] = pd.to_datetime(transformed['timestamp']).dt.date
                        else:
                            transformed[col] = datetime.now().date()
                    elif col == 'migration_source':
                        transformed[col] = 'old_schema'
                    elif col == 'migration_timestamp':
                        transformed[col] = datetime.now(timezone.utc)
                    elif col == 'processing_version':
                        transformed[col] = 'v1.0_migrated'
                    else:
                        transformed[col] = None
            
            # Derive computed fields
            transformed['transaction_type'] = transformed.apply(self.derive_transaction_type, axis=1)
            transformed['value_usd'] = transformed.apply(self.calculate_usd_value, axis=1)
            transformed['data_quality_score'] = transformed.apply(self.calculate_data_quality_score, axis=1)
            
            # Create derived_fields summary
            transformed['derived_fields'] = transformed.apply(
                lambda row: json.dumps({
                    'transaction_type_derived': True,
                    'value_usd_derived': True,
                    'data_quality_calculated': True,
                    'migration_applied': True
                }), axis=1
            )
            
            logger.info(f"Transformed {len(transformed)} rows from old schema")
            return transformed[self.unified_schema]  # Ensure column order
            
        except Exception as e:
            logger.error(f"Error transforming old schema: {e}")
            return pd.DataFrame()

    def transform_current_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform current schema DataFrame to unified schema"""
        try:
            # Current schema should mostly match, just add migration metadata
            transformed = df.copy()
            
            # Add missing columns
            for col in self.unified_schema:
                if col not in transformed.columns:
                    if col == 'migration_source':
                        transformed[col] = 'current_schema'
                    elif col == 'migration_timestamp':
                        transformed[col] = datetime.now(timezone.utc)
                    elif col == 'transaction_type':
                        transformed[col] = transformed.apply(self.derive_transaction_type, axis=1)
                    elif col == 'value_usd':
                        transformed[col] = transformed.apply(self.calculate_usd_value, axis=1)
                    elif col == 'data_quality_score':
                        transformed[col] = transformed.apply(self.calculate_data_quality_score, axis=1)
                    elif col == 'derived_fields':
                        transformed[col] = json.dumps({
                            'migration_applied': True,
                            'schema_current': True
                        })
                    elif col == 'processing_version':
                        transformed[col] = 'v2.0_current'
                    else:
                        transformed[col] = None
            
            logger.info(f"Transformed {len(transformed)} rows from current schema")
            return transformed[self.unified_schema]  # Ensure column order
            
        except Exception as e:
            logger.error(f"Error transforming current schema: {e}")
            return pd.DataFrame()

    def write_unified_parquet(self, df: pd.DataFrame, output_key: str) -> bool:
        """Write unified DataFrame to S3 as parquet"""
        try:
            # Convert to pyarrow table for better control
            table = pa.Table.from_pandas(df)
            
            # Write to buffer
            import io
            buffer = io.BytesIO()
            pq.write_table(table, buffer)
            buffer.seek(0)
            
            # Upload to S3
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=output_key,
                Body=buffer.getvalue(),
                ContentType='application/octet-stream'
            )
            
            logger.info(f"Written {len(df)} rows to {output_key}")
            return True
            
        except Exception as e:
            logger.error(f"Error writing to {output_key}: {e}")
            return False

    def migrate_schema(self, source_type: str, source_files: List[str]) -> Tuple[int, int]:
        """Migrate a set of files from source schema to unified schema"""
        total_files = len(source_files)
        successful_files = 0
        total_rows = 0
        
        for file_key in source_files:
            try:
                logger.info(f"Processing {file_key}")
                
                # Read source file
                df = self.read_parquet_from_s3(file_key)
                if df is None or df.empty:
                    continue
                
                # Transform based on source type
                if source_type == 'old':
                    transformed_df = self.transform_old_schema(df)
                else:
                    transformed_df = self.transform_current_schema(df)
                
                if transformed_df.empty:
                    continue
                
                # Generate output key
                # Extract date partition from original key if possible
                if 'date=' in file_key:
                    date_part = file_key.split('date=')[1].split('/')[0]
                    output_key = f"{self.unified_schema_prefix}date={date_part}/unified_{source_type}_schema_{date_part}.parquet"
                else:
                    # Use timestamp for non-partitioned files
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    output_key = f"{self.unified_schema_prefix}date=unknown/unified_{source_type}_schema_{timestamp}.parquet"
                
                # Write unified file
                if self.write_unified_parquet(transformed_df, output_key):
                    successful_files += 1
                    total_rows += len(transformed_df)
                
            except Exception as e:
                logger.error(f"Error processing {file_key}: {e}")
                continue
        
        return successful_files, total_rows

    def run_migration(self) -> Dict[str, any]:
        """Run the complete migration process"""
        logger.info("Starting bronze wallet transactions schema migration")
        
        results = {
            'start_time': datetime.now(timezone.utc),
            'old_schema': {'files': 0, 'rows': 0},
            'current_schema': {'files': 0, 'rows': 0},
            'total_unified_files': 0,
            'total_unified_rows': 0,
            'success': False,
            'errors': []
        }
        
        try:
            # Process old schema files
            logger.info("Processing old schema files...")
            old_files = self.list_files(self.old_schema_prefix)
            if old_files:
                old_success, old_rows = self.migrate_schema('old', old_files)
                results['old_schema']['files'] = old_success
                results['old_schema']['rows'] = old_rows
            
            # Process current schema files
            logger.info("Processing current schema files...")
            current_files = self.list_files(self.current_schema_prefix)
            # Filter out status files, only process transaction files
            transaction_files = [f for f in current_files if not f.endswith('status.parquet')]
            if transaction_files:
                current_success, current_rows = self.migrate_schema('current', transaction_files)
                results['current_schema']['files'] = current_success
                results['current_schema']['rows'] = current_rows
            
            # Calculate totals
            results['total_unified_files'] = results['old_schema']['files'] + results['current_schema']['files']
            results['total_unified_rows'] = results['old_schema']['rows'] + results['current_schema']['rows']
            
            # Write migration summary
            summary_key = f"{self.unified_schema_prefix}migration_summary.json"
            summary_data = {
                **results,
                'end_time': datetime.now(timezone.utc).isoformat(),
                'unified_schema_columns': self.unified_schema,
                'field_mappings_applied': self.field_mappings
            }
            
            # Convert datetime objects to strings for JSON serialization
            summary_data['start_time'] = summary_data['start_time'].isoformat()
            
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=summary_key,
                Body=json.dumps(summary_data, indent=2),
                ContentType='application/json'
            )
            
            results['success'] = True
            logger.info(f"Migration completed successfully: {results['total_unified_files']} files, {results['total_unified_rows']} rows")
            
        except Exception as e:
            error_msg = f"Migration failed: {e}"
            logger.error(error_msg)
            results['errors'].append(error_msg)
        
        return results


def main():
    """Main function to run the migration"""
    print("Bronze Wallet Transactions Schema Migration")
    print("=" * 50)
    
    # Initialize migrator
    migrator = BronzeWalletTransactionsMigrator()
    
    # Run migration
    results = migrator.run_migration()
    
    # Print results
    print(f"\nMigration Results:")
    print(f"Old Schema Files: {results['old_schema']['files']} files, {results['old_schema']['rows']} rows")
    print(f"Current Schema Files: {results['current_schema']['files']} files, {results['current_schema']['rows']} rows")
    print(f"Total Unified: {results['total_unified_files']} files, {results['total_unified_rows']} rows")
    print(f"Success: {results['success']}")
    
    if results['errors']:
        print(f"Errors: {results['errors']}")
    
    # Exit with appropriate code
    sys.exit(0 if results['success'] else 1)


if __name__ == "__main__":
    main()