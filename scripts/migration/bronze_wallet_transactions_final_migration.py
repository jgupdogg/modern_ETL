#!/usr/bin/env python3
"""
Bronze Wallet Transactions Final Migration Script

Handles the complete schema evolution:
1. Old Schema (wallet_transactions_old_schema): from_symbol/to_symbol format
2. Current Schema v1 (early files): from_symbol/to_symbol format  
3. Current Schema v2 (recent files): base_symbol/quote_symbol format

Creates unified dataset for comprehensive smart trader analysis.

Author: Generated by Claude Code
Date: 2025-06-19
"""

import duckdb
import json
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FinalWalletTransactionsMigrator:
    """Final migrator handling all schema variations"""
    
    def __init__(self):
        """Initialize the comprehensive migrator"""
        
        self.old_schema_path = "s3://solana-data/bronze/wallet_transactions_old_schema/**/*.parquet"
        self.current_schema_base = "s3://solana-data/bronze/wallet_transactions/"
        self.unified_output_path = "s3://solana-data/bronze/wallet_transactions_unified/"
        
        # Initialize DuckDB connection
        self.conn = duckdb.connect()
        
        # Configure MinIO/S3 settings
        self.conn.execute("""
            INSTALL httpfs;
            LOAD httpfs;
            SET s3_endpoint='localhost:9000';
            SET s3_access_key_id='minioadmin';
            SET s3_secret_access_key='minioadmin123';
            SET s3_use_ssl=false;
            SET s3_url_style='path';
        """)
        
        logger.info("Final migrator initialized")

    def identify_schema_variations(self) -> Dict[str, any]:
        """Identify different schema variations in the current data"""
        variations = {
            'old_schema': {
                'path': self.old_schema_path,
                'format': 'from_to_format',
                'files': [],
                'sample_columns': []
            },
            'current_v1_legacy': {
                'format': 'from_to_format', 
                'files': [],
                'sample_columns': []
            },
            'current_v2_new': {
                'format': 'base_quote_format',
                'files': [],
                'sample_columns': []
            }
        }
        
        try:
            # Analyze old schema
            logger.info("Analyzing old schema structure...")
            old_columns = self.conn.execute(f"""
                DESCRIBE SELECT * FROM read_parquet('{self.old_schema_path}') LIMIT 1
            """).fetchall()
            variations['old_schema']['sample_columns'] = [col[0] for col in old_columns]
            
            # Test different current schema patterns
            logger.info("Identifying current schema variations...")
            
            # Try legacy format files (with token_address, from_symbol)
            try:
                legacy_sample = self.conn.execute(f"""
                    SELECT DISTINCT 's3://solana-data/bronze/wallet_transactions/' || 
                           regexp_extract(filename, 'date=([^/]+)', 1) as date_partition
                    FROM (
                        SELECT filename FROM glob('s3://solana-data/bronze/wallet_transactions/**/wallet_transactions_*.parquet')
                    ) 
                    WHERE filename LIKE '%date=2025-06-13%' OR filename LIKE '%date=2025-06-14%'
                    LIMIT 3
                """).fetchall()
                
                if legacy_sample:
                    test_path = f"s3://solana-data/bronze/wallet_transactions/date=2025-06-13/wallet_transactions_*.parquet"
                    legacy_columns = self.conn.execute(f"""
                        DESCRIBE SELECT * FROM read_parquet('{test_path}') LIMIT 1
                    """).fetchall()
                    variations['current_v1_legacy']['sample_columns'] = [col[0] for col in legacy_columns]
                    variations['current_v1_legacy']['files'] = [test_path]
                    
            except Exception as e:
                logger.info(f"No legacy format files found: {e}")
            
            # Try new format files (with base_symbol, quote_symbol)
            try:
                test_path = f"s3://solana-data/bronze/wallet_transactions/date=2025-06-18/wallet_transactions_*.parquet"
                new_columns = self.conn.execute(f"""
                    DESCRIBE SELECT * FROM read_parquet('{test_path}') LIMIT 1
                """).fetchall()
                variations['current_v2_new']['sample_columns'] = [col[0] for col in new_columns]
                variations['current_v2_new']['files'] = [test_path]
                
            except Exception as e:
                logger.info(f"No new format files found: {e}")
            
            # Log findings
            for variant, info in variations.items():
                if info['sample_columns']:
                    logger.info(f"{variant}: {len(info['sample_columns'])} columns")
                    if 'from_symbol' in info['sample_columns']:
                        logger.info(f"  - Uses from_symbol/to_symbol format")
                    if 'base_symbol' in info['sample_columns']:
                        logger.info(f"  - Uses base_symbol/quote_symbol format")
            
        except Exception as e:
            logger.error(f"Error identifying schema variations: {e}")
        
        return variations

    def create_comprehensive_unified_dataset(self) -> Dict[str, any]:
        """Create unified dataset handling all schema variations"""
        result = {
            'success': False,
            'unified_records': 0,
            'unified_wallets': 0,
            'schema_breakdown': {},
            'errors': []
        }
        
        try:
            logger.info("Creating comprehensive unified dataset...")
            
            # Define common unified schema
            unified_columns = """
                wallet_address,
                transaction_hash,
                timestamp,
                block_slot,
                
                -- Standardized token fields (unified from both formats)
                token_a_symbol,
                token_b_symbol, 
                token_a_amount,
                token_b_amount,
                
                -- Price and value fields
                price_usd,
                transaction_value_usd,
                
                -- Transaction metadata
                transaction_type,
                transaction_fee,
                success,
                error_message,
                
                -- Processing metadata
                processed_at,
                batch_id,
                processed_for_pnl,
                pnl_processed_at,
                pnl_processing_batch_id,
                
                -- Migration metadata
                schema_version,
                migration_source,
                migration_timestamp,
                partition_date
            """
            
            # Build unified query for each schema variant
            unified_query_parts = []
            
            # Part 1: Old schema data
            old_schema_query = f"""
            SELECT 
                wallet_address,
                transaction_hash,
                timestamp,
                block_slot,
                
                -- Map from_symbol/to_symbol to unified format
                from_symbol as token_a_symbol,
                to_symbol as token_b_symbol,
                from_amount as token_a_amount,
                to_amount as token_b_amount,
                
                -- Price and value
                COALESCE(nearest_price, base_price, quote_price) as price_usd,
                COALESCE(value_usd, from_amount * COALESCE(nearest_price, 1.0)) as transaction_value_usd,
                
                -- Transaction metadata  
                COALESCE(transaction_type, 'UNKNOWN') as transaction_type,
                COALESCE(transaction_fee, 0.0) as transaction_fee,
                COALESCE(success, true) as success,
                COALESCE(error_message, '') as error_message,
                
                -- Processing metadata
                processed_at,
                batch_id,
                COALESCE(processed_for_pnl, false) as processed_for_pnl,
                pnl_processed_at,
                pnl_processing_batch_id,
                
                -- Migration metadata
                'old_schema_v1' as schema_version,
                'wallet_transactions_old_schema' as migration_source,
                CAST('2025-06-19T23:15:00Z' as TIMESTAMP) as migration_timestamp,
                DATE(timestamp) as partition_date
                
            FROM read_parquet('{self.old_schema_path}')
            """
            unified_query_parts.append(old_schema_query)
            
            # Part 2: Current schema v1 (legacy format with from_symbol/to_symbol)
            current_v1_query = f"""
            SELECT 
                wallet_address,
                transaction_hash,
                timestamp,
                COALESCE(block_slot, block_unix_time) as block_slot,
                
                -- Map from_symbol/to_symbol to unified format
                from_symbol as token_a_symbol,
                to_symbol as token_b_symbol,
                from_amount as token_a_amount,
                to_amount as token_b_amount,
                
                -- Price and value
                COALESCE(nearest_price, base_price, quote_price) as price_usd,
                COALESCE(value_usd, from_amount * COALESCE(nearest_price, 1.0)) as transaction_value_usd,
                
                -- Transaction metadata
                COALESCE(transaction_type, 'UNKNOWN') as transaction_type,
                COALESCE(transaction_fee, 0.0) as transaction_fee,
                COALESCE(success, true) as success,
                COALESCE(error_message, '') as error_message,
                
                -- Processing metadata
                processed_at,
                batch_id,
                COALESCE(processed_for_pnl, false) as processed_for_pnl,
                pnl_processed_at,
                pnl_processing_batch_id,
                
                -- Migration metadata
                'current_schema_v1' as schema_version,
                'wallet_transactions_legacy' as migration_source,
                CAST('2025-06-19T23:15:00Z' as TIMESTAMP) as migration_timestamp,
                DATE(timestamp) as partition_date
                
            FROM read_parquet('s3://solana-data/bronze/wallet_transactions/date=2025-06-13/wallet_transactions_*.parquet', 
                             union_by_name=true)
            """
            # Only add if files exist
            try:
                test_count = self.conn.execute("""
                    SELECT COUNT(*) FROM read_parquet('s3://solana-data/bronze/wallet_transactions/date=2025-06-13/wallet_transactions_*.parquet', union_by_name=true) LIMIT 1
                """).fetchone()[0]
                if test_count > 0:
                    unified_query_parts.append(current_v1_query)
                    logger.info(f"Added current schema v1 with {test_count} records")
            except:
                logger.info("Skipping current schema v1 - no compatible files")
            
            # Part 3: Current schema v2 (new format with base_symbol/quote_symbol)
            current_v2_query = f"""
            SELECT 
                wallet_address,
                transaction_hash,
                timestamp,
                COALESCE(block_unix_time) as block_slot,
                
                -- Map base_symbol/quote_symbol to unified format
                base_symbol as token_a_symbol,
                quote_symbol as token_b_symbol,
                base_ui_amount as token_a_amount,
                quote_ui_amount as token_b_amount,
                
                -- Price and value
                COALESCE(base_nearest_price, quote_nearest_price) as price_usd,
                COALESCE(base_ui_amount * base_nearest_price, quote_ui_amount * quote_nearest_price) as transaction_value_usd,
                
                -- Derive transaction type from swap types
                CASE 
                    WHEN base_type_swap = 'out' AND quote_type_swap = 'in' THEN 'SELL'
                    WHEN base_type_swap = 'in' AND quote_type_swap = 'out' THEN 'BUY'
                    ELSE 'SWAP'
                END as transaction_type,
                0.0 as transaction_fee,  -- Not available in new schema
                true as success,  -- Assume success if no error field
                '' as error_message,
                
                -- Processing metadata
                COALESCE(fetched_at, CURRENT_TIMESTAMP) as processed_at,
                batch_id,
                COALESCE(processed_for_pnl, false) as processed_for_pnl,
                pnl_processed_at,
                pnl_processing_batch_id,
                
                -- Migration metadata
                'current_schema_v2' as schema_version,
                'wallet_transactions_new' as migration_source,
                CAST('2025-06-19T23:15:00Z' as TIMESTAMP) as migration_timestamp,
                DATE(timestamp) as partition_date
                
            FROM read_parquet('s3://solana-data/bronze/wallet_transactions/date=2025-06-18/wallet_transactions_*.parquet',
                             union_by_name=true)
            """
            # Only add if files exist
            try:
                test_count = self.conn.execute("""
                    SELECT COUNT(*) FROM read_parquet('s3://solana-data/bronze/wallet_transactions/date=2025-06-18/wallet_transactions_*.parquet', union_by_name=true) LIMIT 1
                """).fetchone()[0]
                if test_count > 0:
                    unified_query_parts.append(current_v2_query)
                    logger.info(f"Added current schema v2 with {test_count} records")
            except:
                logger.info("Skipping current schema v2 - no compatible files")
            
            # Combine all parts
            if not unified_query_parts:
                result['errors'].append("No compatible schema files found")
                return result
                
            full_unified_query = " UNION ALL ".join(unified_query_parts)
            
            # Get statistics
            logger.info("Getting unified dataset statistics...")
            unified_stats = self.conn.execute(f"""
                WITH unified_data AS ({full_unified_query})
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT wallet_address) as unique_wallets,
                    COUNT(DISTINCT transaction_hash) as unique_transactions,
                    COUNT(CASE WHEN schema_version = 'old_schema_v1' THEN 1 END) as old_records,
                    COUNT(CASE WHEN schema_version = 'current_schema_v1' THEN 1 END) as current_v1_records,
                    COUNT(CASE WHEN schema_version = 'current_schema_v2' THEN 1 END) as current_v2_records,
                    MIN(timestamp) as earliest_transaction,
                    MAX(timestamp) as latest_transaction,
                    AVG(transaction_value_usd) as avg_value_usd,
                    MAX(transaction_value_usd) as max_value_usd
                FROM unified_data
            """).fetchone()
            
            result['unified_records'] = unified_stats[0]
            result['unified_wallets'] = unified_stats[1]
            result['unique_transactions'] = unified_stats[2]
            result['schema_breakdown'] = {
                'old_schema_v1': unified_stats[3],
                'current_schema_v1': unified_stats[4], 
                'current_schema_v2': unified_stats[5]
            }
            result['time_range'] = f"{unified_stats[6]} to {unified_stats[7]}"
            result['avg_value_usd'] = round(unified_stats[8], 4) if unified_stats[8] else 0
            result['max_value_usd'] = round(unified_stats[9], 4) if unified_stats[9] else 0
            
            # Write unified dataset
            logger.info("Writing comprehensive unified dataset...")
            
            self.conn.execute(f"""
                COPY (
                    WITH unified_data AS ({full_unified_query}),
                    deduped AS (
                        SELECT *,
                               ROW_NUMBER() OVER (PARTITION BY wallet_address, transaction_hash ORDER BY timestamp DESC) as rn
                        FROM unified_data
                    )
                    SELECT 
                        wallet_address, transaction_hash, timestamp, block_slot,
                        token_a_symbol, token_b_symbol, token_a_amount, token_b_amount,
                        price_usd, transaction_value_usd, transaction_type, transaction_fee,
                        success, error_message, processed_at, batch_id, processed_for_pnl,
                        pnl_processed_at, pnl_processing_batch_id, schema_version,
                        migration_source, migration_timestamp, partition_date
                    FROM deduped 
                    WHERE rn = 1
                    ORDER BY timestamp, wallet_address
                ) TO '{self.unified_output_path}' (
                    FORMAT PARQUET,
                    PARTITION_BY partition_date,
                    OVERWRITE_OR_IGNORE true
                )
            """)
            
            # Write comprehensive summary
            summary_data = {
                'migration_timestamp': datetime.now(timezone.utc).isoformat(),
                'unified_statistics': {
                    'total_records': result['unified_records'],
                    'unique_wallets': result['unified_wallets'],
                    'unique_transactions': result['unique_transactions'],
                    'schema_breakdown': result['schema_breakdown'],
                    'time_range': result['time_range'],
                    'avg_value_usd': result['avg_value_usd'],
                    'max_value_usd': result['max_value_usd']
                },
                'migration_notes': {
                    'schemas_processed': len(unified_query_parts),
                    'deduplication_applied': True,
                    'partitioned_by_date': True,
                    'unified_schema_columns': 23,
                    'ready_for_pnl_analysis': True
                }
            }
            
            summary_json = json.dumps(summary_data, indent=2)
            self.conn.execute(f"""
                COPY (SELECT '{summary_json}' as summary) 
                TO '{self.unified_output_path}migration_summary.json' 
                (FORMAT JSON, OVERWRITE_OR_IGNORE true)
            """)
            
            result['success'] = True
            logger.info(f"Comprehensive unified dataset created successfully")
            
        except Exception as e:
            error_msg = f"Error creating unified dataset: {e}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
        
        return result

    def run_final_migration(self) -> Dict[str, any]:
        """Run the final comprehensive migration"""
        logger.info("Starting final comprehensive migration")
        
        migration_result = {
            'start_time': datetime.now(timezone.utc).isoformat(),
            'schema_variations': {},
            'unified_dataset': {},
            'success': False,
            'errors': []
        }
        
        try:
            # Step 1: Identify schema variations
            migration_result['schema_variations'] = self.identify_schema_variations()
            
            # Step 2: Create comprehensive unified dataset
            migration_result['unified_dataset'] = self.create_comprehensive_unified_dataset()
            migration_result['success'] = migration_result['unified_dataset']['success']
            
            migration_result['end_time'] = datetime.now(timezone.utc).isoformat()
            
        except Exception as e:
            error_msg = f"Final migration failed: {e}"
            logger.error(error_msg)
            migration_result['errors'].append(error_msg)
        
        finally:
            self.conn.close()
        
        return migration_result


def main():
    """Main function to run the final migration"""
    print("Bronze Wallet Transactions Final Migration")
    print("Handling all schema variations for comprehensive analysis")
    print("=" * 60)
    
    # Initialize migrator
    migrator = FinalWalletTransactionsMigrator()
    
    # Run final migration
    results = migrator.run_final_migration()
    
    # Print results
    print(f"\nFinal Migration Results:")
    print(f"Success: {results['success']}")
    
    if 'unified_dataset' in results and results['unified_dataset'].get('success'):
        unified = results['unified_dataset']
        print(f"\nUnified Dataset Created:")
        print(f"  Total Records: {unified.get('unified_records', 0):,}")
        print(f"  Unique Wallets: {unified.get('unified_wallets', 0):,}")
        print(f"  Unique Transactions: {unified.get('unique_transactions', 0):,}")
        print(f"  Time Range: {unified.get('time_range', 'Unknown')}")
        print(f"  Avg Transaction Value: ${unified.get('avg_value_usd', 0):,.2f}")
        
        if 'schema_breakdown' in unified:
            breakdown = unified['schema_breakdown']
            print(f"\nSchema Breakdown:")
            print(f"  Old Schema: {breakdown.get('old_schema_v1', 0):,} records")
            print(f"  Current Schema v1: {breakdown.get('current_schema_v1', 0):,} records")
            print(f"  Current Schema v2: {breakdown.get('current_schema_v2', 0):,} records")
        
        print(f"\nOutput Location: s3://solana-data/bronze/wallet_transactions_unified/")
        print(f"Ready for comprehensive smart trader analysis!")
    
    if results.get('errors'):
        print(f"\nErrors:")
        for error in results['errors']:
            print(f"  - {error}")
    
    # Exit with appropriate code
    sys.exit(0 if results['success'] else 1)


if __name__ == "__main__":
    main()