#!/usr/bin/env python3
"""
Fixed Bronze Wallet Trade History Migration

Corrected logic to process ALL 3.6M records in proper batches.
The previous version had a bug where it stopped after the first batch.

Author: Generated by Claude Code
Date: 2025-06-19
"""

import os
import logging
import json
import time
import psycopg2
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import io
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FixedBronzeMigrator:
    """Fixed migrator with correct batch processing logic"""
    
    def __init__(self):
        self.pg_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'database': 'solana_pipeline',
            'user': os.getenv('DB_USER', 'postgres'),
            'password': os.getenv('DB_PASSWORD', 'St0ck!adePG')
        }
        
        self.s3_client = boto3.client(
            's3',
            endpoint_url='http://localhost:9000',
            aws_access_key_id='minioadmin',
            aws_secret_access_key='minioadmin123',
            region_name='us-east-1'
        )
        
        self.bucket = 'solana-data'
        self.path = 'bronze/wallet_trade_history_bronze_fixed/'
        self.batch_size = 25000  # Smaller batches for better memory management
        
    def get_total_count(self):
        """Get total record count"""
        with psycopg2.connect(**self.pg_config) as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM bronze.wallet_trade_history;")
                return cur.fetchone()[0]
    
    def process_batch_by_offset(self, offset: int, limit: int, batch_num: int):
        """Process a batch using OFFSET/LIMIT approach"""
        try:
            with psycopg2.connect(**self.pg_config) as conn:
                # Simple query with OFFSET/LIMIT
                query = f"""
                SELECT 
                    wallet_address,
                    transaction_hash,
                    COALESCE(timestamp, created_at) as timestamp,
                    
                    -- Map from_symbol/to_symbol to unified format
                    from_symbol as token_a,
                    to_symbol as token_b,
                    COALESCE(from_amount, 0) as amount_a,
                    COALESCE(to_amount, 0) as amount_b,
                    
                    -- USD calculation with NULL improvement
                    CASE 
                        WHEN value_usd IS NOT NULL AND value_usd > 0 THEN value_usd
                        WHEN base_price > 0 AND from_amount > 0 THEN from_amount * base_price
                        WHEN quote_price > 0 AND to_amount > 0 THEN to_amount * quote_price
                        ELSE 0
                    END as value_usd,
                    
                    -- Price fields
                    COALESCE(base_price, 0) as base_price,
                    COALESCE(quote_price, 0) as quote_price,
                    0 as nearest_price,
                    
                    -- Transaction type
                    COALESCE(transaction_type, 'UNKNOWN') as transaction_type,
                    
                    -- Processing fields
                    COALESCE(processed_for_pnl, false) as processed_for_pnl,
                    
                    -- Token details
                    from_address as token_a_address,
                    to_address as token_b_address,
                    
                    -- Migration metadata
                    'bronze_fixed_migration' as migration_source,
                    DATE(COALESCE(timestamp, created_at)) as partition_date
                    
                FROM bronze.wallet_trade_history
                ORDER BY id
                OFFSET {offset}
                LIMIT {limit};
                """
                
                df = pd.read_sql_query(query, conn)
                
                if df.empty:
                    return 0
                
                # Upload to MinIO by date partition
                records_uploaded = 0
                for date, date_df in df.groupby('partition_date'):
                    if len(date_df) == 0:
                        continue
                    
                    # Create parquet
                    table = pa.Table.from_pandas(date_df)
                    buffer = io.BytesIO()
                    pq.write_table(table, buffer, compression='snappy')
                    buffer.seek(0)
                    
                    # Upload
                    date_str = date.strftime('%Y-%m-%d') if date else 'unknown'
                    key = f"{self.path}date={date_str}/batch_{batch_num:06d}_{int(time.time())}.parquet"
                    
                    self.s3_client.put_object(
                        Bucket=self.bucket,
                        Key=key,
                        Body=buffer.getvalue(),
                        ContentType='application/octet-stream'
                    )
                    
                    records_uploaded += len(date_df)
                
                logger.info(f"Batch {batch_num}: Processed {len(df):,} records (offset {offset:,})")
                return len(df)
                
        except Exception as e:
            logger.error(f"Batch {batch_num} failed: {e}")
            return 0
    
    def run_fixed_migration(self):
        """Run migration with fixed logic"""
        print("\n" + "="*60)
        print("FIXED BRONZE MIGRATION - ALL 3.6M RECORDS")
        print("="*60)
        
        # Get total count
        total_count = self.get_total_count()
        total_batches = (total_count + self.batch_size - 1) // self.batch_size
        
        print(f"\nðŸ“Š MIGRATION PLAN:")
        print(f"  Total Records: {total_count:,}")
        print(f"  Batch Size: {self.batch_size:,}")
        print(f"  Total Batches: {total_batches:,}")
        
        # Process all batches
        print(f"\nðŸš€ PROCESSING ALL BATCHES...")
        start_time = time.time()
        total_processed = 0
        
        for batch_num in range(total_batches):
            offset = batch_num * self.batch_size
            batch_start = time.time()
            
            records_in_batch = self.process_batch_by_offset(offset, self.batch_size, batch_num + 1)
            
            if records_in_batch == 0:
                logger.info(f"Batch {batch_num + 1}: No more records, stopping")
                break
            
            total_processed += records_in_batch
            elapsed = time.time() - start_time
            batch_time = time.time() - batch_start
            
            progress = (batch_num + 1) / total_batches * 100
            rate = total_processed / elapsed if elapsed > 0 else 0
            eta_seconds = (total_count - total_processed) / rate if rate > 0 else 0
            eta_mins = eta_seconds / 60
            
            print(f"  âœ… Batch {batch_num + 1:3d}/{total_batches} | "
                  f"{progress:5.1f}% | "
                  f"{records_in_batch:,} records | "
                  f"{batch_time:.1f}s | "
                  f"Rate: {rate:,.0f}/sec | "
                  f"ETA: {eta_mins:.1f}min")
            
            # Brief pause to prevent system overload
            time.sleep(0.1)
        
        # Final results
        total_time = time.time() - start_time
        print(f"\nâœ… MIGRATION COMPLETED!")
        print(f"ðŸ“Š FINAL STATISTICS:")
        print(f"  Total Processed: {total_processed:,}")
        print(f"  Total Time: {total_time/60:.1f} minutes")
        print(f"  Average Rate: {total_processed/total_time:,.0f} records/second")
        print(f"  Output: s3://{self.bucket}/{self.path}")
        
        return total_processed == total_count

if __name__ == "__main__":
    migrator = FixedBronzeMigrator()
    success = migrator.run_fixed_migration()
    exit(0 if success else 1)