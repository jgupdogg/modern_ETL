claude_pipeline:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: '/data/analytics.duckdb'
      extensions:
        - httpfs
      settings:
        s3_endpoint: 'minio:9000'
        s3_access_key_id: 'minioadmin'
        s3_secret_access_key: 'minioadmin123'
        s3_use_ssl: false
        s3_url_style: 'path'
    
    # PySpark profile for Delta Lake transformations
    spark:
      type: spark
      method: session
      schema: silver_schema
      threads: 4
      # Use existing Spark session with Delta Lake support
      spark_config:
        spark.jars.packages: "io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4"
        spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
        spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
        spark.hadoop.fs.s3a.endpoint: "http://minio:9000"
        spark.hadoop.fs.s3a.access.key: "minioadmin"
        spark.hadoop.fs.s3a.secret.key: "minioadmin123"
        spark.hadoop.fs.s3a.path.style.access: "true"
        spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"