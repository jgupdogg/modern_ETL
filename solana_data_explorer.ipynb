{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solana Data Explorer\n",
    "\n",
    "This notebook explores the migrated Solana tables from PostgreSQL to MinIO storage.\n",
    "\n",
    "## Tables Available:\n",
    "- `token_list_v3` - Token data from BirdEye API\n",
    "- `token_whales` - Large holders for tracked tokens\n",
    "- `wallet_trade_history` - Trading history for whale wallets\n",
    "- `token_metadata` - Token metadata (Twitter, website, descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (923040672.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ---------------------------------------------------------------------------\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "---------------------------------------------------------------------------\n",
    "ImportError                               Traceback (most recent call last)\n",
    "Cell In[1], line 3\n",
    "      1 # Import required libraries\n",
    "      2 import duckdb\n",
    "----> 3 import pandas as pd\n",
    "      4 import os\n",
    "      5 import json\n",
    "\n",
    "File ~/.local/lib/python3.11/site-packages/pandas/__init__.py:19\n",
    "     16         _missing_dependencies.append(f\"{_dependency}: {_e}\")\n",
    "     18 if _missing_dependencies:  # pragma: no cover\n",
    "---> 19     raise ImportError(\n",
    "     20         \"Unable to import required dependencies:\\n\" + \"\\n\".join(_missing_dependencies)\n",
    "     21     )\n",
    "     22 del _hard_dependencies, _dependency, _missing_dependencies\n",
    "     24 try:\n",
    "     25     # numpy compat\n",
    "\n",
    "ImportError: Unable to import required dependencies:\n",
    "numpy: Error importing numpy: you should not try to import numpy from\n",
    "        its source directory; please exit the numpy source tree, and relaunch\n",
    "        your python interpreter from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mduckdb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/jgupdogg/.local/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/lib/python3/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jgupdogg/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jgupdogg/.local/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jgupdogg/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DuckDB connection with S3/MinIO configuration\n",
    "def get_duckdb_connection():\n",
    "    \"\"\"Create DuckDB connection with S3 configuration.\"\"\"\n",
    "    db_path = '/data/analytics.duckdb'\n",
    "    conn = duckdb.connect(db_path)\n",
    "    \n",
    "    # Configure S3/MinIO\n",
    "    conn.execute(\"LOAD httpfs;\")\n",
    "    conn.execute(\"SET s3_endpoint='minio:9000';\")\n",
    "    conn.execute(\"SET s3_access_key_id='minioadmin';\")\n",
    "    conn.execute(\"SET s3_secret_access_key='minioadmin123';\")\n",
    "    conn.execute(\"SET s3_use_ssl=false;\")\n",
    "    conn.execute(\"SET s3_url_style='path';\")\n",
    "    \n",
    "    return conn\n",
    "\n",
    "# Initialize connection\n",
    "conn = get_duckdb_connection()\n",
    "print(\"✅ DuckDB connection established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table paths\n",
    "tables = {\n",
    "    'token_list_v3': 's3://solana-data/bronze/token_list_v3/*.parquet',\n",
    "    'token_whales': 's3://solana-data/bronze/token_whales/*.parquet',\n",
    "    'wallet_trade_history': 's3://solana-data/bronze/wallet_trade_history/*.parquet',\n",
    "    'token_metadata': 's3://solana-data/bronze/token_metadata/*.parquet'\n",
    "}\n",
    "\n",
    "# Get row counts for each table\n",
    "print(\"=== TABLE SUMMARY ===\")\n",
    "row_counts = {}\n",
    "for table_name, path in tables.items():\n",
    "    try:\n",
    "        count = conn.execute(f\"SELECT COUNT(*) FROM read_parquet('{path}')\").fetchone()[0]\n",
    "        row_counts[table_name] = count\n",
    "        print(f\"{table_name}: {count:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table_name}: Error - {e}\")\n",
    "        row_counts[table_name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize table sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "table_names = list(row_counts.keys())\n",
    "counts = list(row_counts.values())\n",
    "\n",
    "bars = plt.bar(table_names, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "plt.title('Solana Tables - Row Counts', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Tables', fontsize=12)\n",
    "plt.ylabel('Number of Rows', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "             str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token List Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token list data\n",
    "token_df = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('s3://solana-data/bronze/token_list_v3/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== TOKEN LIST DATA ===\")\n",
    "print(f\"Shape: {token_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in token_df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "print(\"\\nSample data:\")\n",
    "display(token_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token analysis\n",
    "print(\"=== TOKEN ANALYSIS ===\")\n",
    "print(f\"Total tokens: {len(token_df)}\")\n",
    "print(f\"Unique symbols: {token_df['symbol'].nunique()}\")\n",
    "\n",
    "if 'market_cap' in token_df.columns:\n",
    "    print(f\"\\nMarket Cap Summary:\")\n",
    "    print(token_df['market_cap'].describe())\n",
    "    \n",
    "if 'price' in token_df.columns:\n",
    "    print(f\"\\nPrice Summary:\")\n",
    "    print(token_df['price'].describe())\n",
    "\n",
    "print(\"\\nToken Details:\")\n",
    "for _, row in token_df.iterrows():\n",
    "    print(f\"  {row['symbol']}: {row['name']} - ${row.get('price', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Whales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whale data\n",
    "whales_df = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('s3://solana-data/bronze/token_whales/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== TOKEN WHALES DATA ===\")\n",
    "print(f\"Shape: {whales_df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "display(whales_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whale analysis\n",
    "print(\"=== WHALE ANALYSIS ===\")\n",
    "print(f\"Total whale positions: {len(whales_df)}\")\n",
    "print(f\"Unique wallets: {whales_df['wallet_address'].nunique()}\")\n",
    "print(f\"Unique tokens: {whales_df['token_address'].nunique()}\")\n",
    "\n",
    "# Holdings by token\n",
    "print(\"\\nHoldings by Token:\")\n",
    "token_holdings = whales_df.groupby('token_address').agg({\n",
    "    'holdings_value': ['sum', 'count'],\n",
    "    'holdings_pct': 'sum'\n",
    "}).round(2)\n",
    "display(token_holdings)\n",
    "\n",
    "# Top whales by value\n",
    "print(\"\\nTop Whale Positions:\")\n",
    "top_whales = whales_df.nlargest(3, 'holdings_value')[['wallet_address', 'token_address', 'holdings_value', 'holdings_pct']]\n",
    "for _, whale in top_whales.iterrows():\n",
    "    print(f\"  {whale['wallet_address'][:8]}... holds ${whale['holdings_value']:,.2f} ({whale['holdings_pct']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize whale holdings\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Subplot 1: Holdings by token\n",
    "plt.subplot(2, 2, 1)\n",
    "token_totals = whales_df.groupby('token_address')['holdings_value'].sum()\n",
    "token_labels = [addr[:8] + '...' for addr in token_totals.index]\n",
    "plt.pie(token_totals.values, labels=token_labels, autopct='%1.1f%%')\n",
    "plt.title('Holdings Value by Token')\n",
    "\n",
    "# Subplot 2: Holdings percentage distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(whales_df['holdings_pct'], bins=10, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Holdings Percentage')\n",
    "plt.xlabel('Holdings %')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot 3: Holdings value distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(whales_df['holdings_value'], bins=10, alpha=0.7, color='lightcoral')\n",
    "plt.title('Distribution of Holdings Value')\n",
    "plt.xlabel('Holdings Value ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot 4: Whale positions by token\n",
    "plt.subplot(2, 2, 4)\n",
    "token_counts = whales_df['token_address'].value_counts()\n",
    "token_labels = [addr[:8] + '...' for addr in token_counts.index]\n",
    "plt.bar(range(len(token_counts)), token_counts.values, color='lightgreen')\n",
    "plt.title('Number of Whale Positions by Token')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Number of Positions')\n",
    "plt.xticks(range(len(token_counts)), token_labels, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trading history\n",
    "trades_df = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('s3://solana-data/bronze/wallet_trade_history/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== TRADING HISTORY DATA ===\")\n",
    "print(f\"Shape: {trades_df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "display(trades_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading analysis\n",
    "print(\"=== TRADING ANALYSIS ===\")\n",
    "print(f\"Total trades: {len(trades_df)}\")\n",
    "print(f\"Unique wallets: {trades_df['wallet_address'].nunique()}\")\n",
    "print(f\"Unique signatures: {trades_df['signature'].nunique()}\")\n",
    "\n",
    "if 'usd_value' in trades_df.columns:\n",
    "    print(f\"\\nTrading Volume:\")\n",
    "    print(f\"  Total USD value: ${trades_df['usd_value'].sum():,.2f}\")\n",
    "    print(f\"  Average trade size: ${trades_df['usd_value'].mean():,.2f}\")\n",
    "    print(f\"  Largest trade: ${trades_df['usd_value'].max():,.2f}\")\n",
    "\n",
    "# Token pair analysis\n",
    "print(\"\\nTrading Pairs:\")\n",
    "for _, trade in trades_df.iterrows():\n",
    "    from_token = trade.get('from_token_symbol', 'N/A')\n",
    "    to_token = trade.get('to_token_symbol', 'N/A')\n",
    "    usd_val = trade.get('usd_value', 0)\n",
    "    print(f\"  {from_token} → {to_token}: ${usd_val:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Metadata Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_df = conn.execute(\"\"\"\n",
    "    SELECT * FROM read_parquet('s3://solana-data/bronze/token_metadata/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== TOKEN METADATA ===\")\n",
    "print(f\"Shape: {metadata_df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "display(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata analysis\n",
    "print(\"=== METADATA ANALYSIS ===\")\n",
    "print(f\"Total tokens with metadata: {len(metadata_df)}\")\n",
    "\n",
    "# Check data completeness\n",
    "completeness = {}\n",
    "for col in ['twitter', 'website', 'description', 'coingecko_id']:\n",
    "    if col in metadata_df.columns:\n",
    "        non_null = metadata_df[col].notna().sum()\n",
    "        completeness[col] = f\"{non_null}/{len(metadata_df)} ({non_null/len(metadata_df)*100:.1f}%)\"\n",
    "\n",
    "print(\"\\nData Completeness:\")\n",
    "for field, stats in completeness.items():\n",
    "    print(f\"  {field}: {stats}\")\n",
    "\n",
    "print(\"\\nToken Details:\")\n",
    "for _, token in metadata_df.iterrows():\n",
    "    print(f\"\\n{token['symbol']} - {token['name']}\")\n",
    "    if pd.notna(token.get('description')):\n",
    "        desc = token['description'][:100] + '...' if len(token['description']) > 100 else token['description']\n",
    "        print(f\"  Description: {desc}\")\n",
    "    if pd.notna(token.get('website')):\n",
    "        print(f\"  Website: {token['website']}\")\n",
    "    if pd.notna(token.get('twitter')):\n",
    "        print(f\"  Twitter: {token['twitter']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Table Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tokens with metadata for enriched view\n",
    "enriched_tokens = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        t.symbol,\n",
    "        t.name,\n",
    "        t.market_cap,\n",
    "        t.price,\n",
    "        t.volume_24h,\n",
    "        m.website,\n",
    "        m.twitter,\n",
    "        m.coingecko_id\n",
    "    FROM read_parquet('s3://solana-data/bronze/token_list_v3/*.parquet') t\n",
    "    LEFT JOIN read_parquet('s3://solana-data/bronze/token_metadata/*.parquet') m\n",
    "        ON t.token_address = m.token_address\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== ENRICHED TOKEN VIEW ===\")\n",
    "display(enriched_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whale positions with token info\n",
    "whale_positions = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        w.wallet_address,\n",
    "        t.symbol,\n",
    "        t.name,\n",
    "        w.holdings_amount,\n",
    "        w.holdings_value,\n",
    "        w.holdings_pct,\n",
    "        t.price\n",
    "    FROM read_parquet('s3://solana-data/bronze/token_whales/*.parquet') w\n",
    "    LEFT JOIN read_parquet('s3://solana-data/bronze/token_list_v3/*.parquet') t\n",
    "        ON w.token_address = t.token_address\n",
    "    ORDER BY w.holdings_value DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"=== WHALE POSITIONS WITH TOKEN INFO ===\")\n",
    "display(whale_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "print(f\"Migration Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total Tables: {len(tables)}\")\n",
    "print(f\"Total Records: {sum(row_counts.values()):,}\")\n",
    "\n",
    "print(\"\\nTable Status:\")\n",
    "for table, count in row_counts.items():\n",
    "    status = \"✅\" if count > 0 else \"❌\"\n",
    "    print(f\"  {status} {table}: {count:,} rows\")\n",
    "\n",
    "# Test data integrity\n",
    "print(\"\\nData Integrity Checks:\")\n",
    "\n",
    "# Check for valid token addresses\n",
    "invalid_addresses = conn.execute(\"\"\"\n",
    "    SELECT COUNT(*) FROM read_parquet('s3://solana-data/bronze/token_list_v3/*.parquet')\n",
    "    WHERE token_address IS NULL OR token_address = ''\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"  Token addresses - Invalid: {invalid_addresses}\")\n",
    "\n",
    "# Check for valid whale holdings\n",
    "negative_holdings = conn.execute(\"\"\"\n",
    "    SELECT COUNT(*) FROM read_parquet('s3://solana-data/bronze/token_whales/*.parquet')\n",
    "    WHERE holdings_amount < 0 OR holdings_value < 0\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"  Whale holdings - Negative values: {negative_holdings}\")\n",
    "\n",
    "# Check for future trade dates\n",
    "future_trades = conn.execute(f\"\"\"\n",
    "    SELECT COUNT(*) FROM read_parquet('s3://solana-data/bronze/wallet_trade_history/*.parquet')\n",
    "    WHERE block_time > '{datetime.now().isoformat()}'\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"  Trade dates - Future dates: {future_trades}\")\n",
    "\n",
    "print(\"\\n🎉 Data exploration complete!\")\n",
    "print(\"All Solana tables successfully migrated and analyzed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()\n",
    "print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
