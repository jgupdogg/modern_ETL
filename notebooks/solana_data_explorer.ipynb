{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solana Data Explorer - Delta Lake Tables\n",
    "\n",
    "Simple viewer for the most recent data in each Delta Lake table from our DAG pipeline.\n",
    "\n",
    "## Tables:\n",
    "- **Bronze**: token_metrics, whale_holders, transaction_history\n",
    "- **Silver**: tracked_tokens_delta, tracked_whales_delta, wallet_pnl\n",
    "- **Gold**: smart_traders_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(\"üìä Solana Data Explorer - Delta Lake Edition\")\n",
    "print(f\"üïê Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with Delta Lake support\n",
    "print(\"üöÄ Initializing Spark with Delta Lake...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SolanaDataExplorer\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark session ready with Delta Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•â Bronze Layer Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Token Metrics\n",
    "print(\"üì¶ BRONZE: Token Metrics (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    bronze_tokens_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/bronze/token_metrics\")\n",
    "    \n",
    "    # Get record count and latest 10 records\n",
    "    total_count = bronze_tokens_df.count()\n",
    "    latest_tokens = bronze_tokens_df.orderBy(\"_delta_timestamp\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 tokens:\")\n",
    "    display_cols = ['token_address', 'symbol', 'liquidity', 'price', 'processing_date', '_delta_timestamp']\n",
    "    print(latest_tokens[[col for col in display_cols if col in latest_tokens.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading bronze tokens: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Whale Holders\n",
    "print(\"\\nüì¶ BRONZE: Whale Holders (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    bronze_whales_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/bronze/whale_holders\")\n",
    "    \n",
    "    total_count = bronze_whales_df.count()\n",
    "    latest_whales = bronze_whales_df.orderBy(\"_delta_timestamp\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 whale holders:\")\n",
    "    display_cols = ['wallet_address', 'token_symbol', 'rank', 'holdings_amount', 'holdings_value_usd', '_delta_timestamp']\n",
    "    print(latest_whales[[col for col in display_cols if col in latest_whales.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading bronze whales: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze Transaction History\n",
    "print(\"\\nüì¶ BRONZE: Transaction History (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    bronze_txns_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/bronze/transaction_history\")\n",
    "    \n",
    "    total_count = bronze_txns_df.count()\n",
    "    latest_txns = bronze_txns_df.orderBy(\"timestamp\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 transactions:\")\n",
    "    display_cols = ['wallet_address', 'timestamp', 'base_symbol', 'quote_symbol', 'base_type_swap', 'quote_type_swap']\n",
    "    print(latest_txns[[col for col in display_cols if col in latest_txns.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading bronze transactions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•à Silver Layer Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Tracked Tokens\n",
    "print(\"üíé SILVER: Tracked Tokens (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    silver_tokens_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/silver/tracked_tokens_delta\")\n",
    "    \n",
    "    total_count = silver_tokens_df.count()\n",
    "    latest_tokens = silver_tokens_df.orderBy(\"silver_created_at\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 tracked tokens:\")\n",
    "    display_cols = ['token_address', 'symbol', 'liquidity', 'liquidity_tier', 'whale_fetch_status', 'silver_created_at']\n",
    "    print(latest_tokens[[col for col in display_cols if col in latest_tokens.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading silver tokens: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Tracked Whales\n",
    "print(\"\\nüíé SILVER: Tracked Whales (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    silver_whales_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/silver/tracked_whales_delta\")\n",
    "    \n",
    "    total_count = silver_whales_df.count()\n",
    "    latest_whales = silver_whales_df.orderBy(\"silver_created_at\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 tracked whales:\")\n",
    "    display_cols = ['whale_id', 'wallet_address', 'token_symbol', 'whale_tier', 'processing_status', 'silver_created_at']\n",
    "    print(latest_whales[[col for col in display_cols if col in latest_whales.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading silver whales: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver Wallet PnL\n",
    "print(\"\\nüíé SILVER: Wallet PnL (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    silver_pnl_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/silver/wallet_pnl\")\n",
    "    \n",
    "    # Filter for portfolio-level records\n",
    "    portfolio_df = silver_pnl_df.filter(silver_pnl_df.token_address == \"ALL_TOKENS\")\n",
    "    \n",
    "    total_count = portfolio_df.count()\n",
    "    latest_pnl = portfolio_df.orderBy(\"processed_at\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total portfolio records: {total_count:,}\")\n",
    "    print(f\"\\nLatest 10 wallet PnL records:\")\n",
    "    display_cols = ['wallet_address', 'total_pnl', 'portfolio_roi', 'win_rate', 'trade_count', 'processed_at']\n",
    "    print(latest_pnl[[col for col in display_cols if col in latest_pnl.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading silver PnL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•á Gold Layer Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Smart Traders\n",
    "print(\"üèÜ GOLD: Smart Traders (Latest 10 records)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    gold_traders_df = spark.read.format(\"delta\").load(\"s3a://smart-trader/gold/smart_traders_delta\")\n",
    "    \n",
    "    total_count = gold_traders_df.count()\n",
    "    latest_traders = gold_traders_df.orderBy(\"total_pnl\", ascending=False).limit(10).toPandas()\n",
    "    \n",
    "    print(f\"Total smart traders: {total_count:,}\")\n",
    "    \n",
    "    # Get tier breakdown\n",
    "    tier_counts = gold_traders_df.groupBy(\"performance_tier\").count().toPandas()\n",
    "    print(f\"\\nPerformance tiers:\")\n",
    "    for _, row in tier_counts.iterrows():\n",
    "        print(f\"  {row['performance_tier']}: {row['count']:,}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 smart traders by PnL:\")\n",
    "    display_cols = ['wallet_address', 'total_pnl', 'roi', 'win_rate', 'performance_tier', 'gold_processed_at']\n",
    "    print(latest_traders[[col for col in display_cols if col in latest_traders.columns]])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading gold traders: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Summary\n",
    "print(\"\\nüìä PIPELINE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect all table stats\n",
    "table_stats = []\n",
    "\n",
    "tables = [\n",
    "    (\"Bronze Token Metrics\", \"s3a://smart-trader/bronze/token_metrics\"),\n",
    "    (\"Bronze Whale Holders\", \"s3a://smart-trader/bronze/whale_holders\"),\n",
    "    (\"Bronze Transactions\", \"s3a://smart-trader/bronze/transaction_history\"),\n",
    "    (\"Silver Tracked Tokens\", \"s3a://smart-trader/silver/tracked_tokens_delta\"),\n",
    "    (\"Silver Tracked Whales\", \"s3a://smart-trader/silver/tracked_whales_delta\"),\n",
    "    (\"Silver Wallet PnL\", \"s3a://smart-trader/silver/wallet_pnl\"),\n",
    "    (\"Gold Smart Traders\", \"s3a://smart-trader/gold/smart_traders_delta\")\n",
    "]\n",
    "\n",
    "for table_name, table_path in tables:\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(table_path)\n",
    "        count = df.count()\n",
    "        \n",
    "        # Get Delta table history\n",
    "        from delta.tables import DeltaTable\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        history = delta_table.history(1).collect()\n",
    "        latest_version = history[0]['version'] if history else 0\n",
    "        \n",
    "        table_stats.append({\n",
    "            \"Table\": table_name,\n",
    "            \"Records\": f\"{count:,}\",\n",
    "            \"Version\": latest_version,\n",
    "            \"Status\": \"‚úÖ Active\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        table_stats.append({\n",
    "            \"Table\": table_name,\n",
    "            \"Records\": \"0\",\n",
    "            \"Version\": \"N/A\",\n",
    "            \"Status\": \"‚ùå Missing\"\n",
    "        })\n",
    "\n",
    "# Display summary\n",
    "summary_df = pd.DataFrame(table_stats)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úÖ Data exploration complete!\")\n",
    "print(f\"üïê {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session\n",
    "spark.stop()\n",
    "print(\"üõë Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}